{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hits-sdo/hits-sdo-similaritysearch/blob/ss_dataloader/search_simsiam/simsiam_example_notebook_HITS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hits-sdo/hits-sdo-similaritysearch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpKcrCT_4Myq",
        "outputId": "07cf32cf-81f7-43aa-8380-e815fd97dd9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hits-sdo-similaritysearch'...\n",
            "remote: Enumerating objects: 309, done.\u001b[K\n",
            "remote: Counting objects: 100% (309/309), done.\u001b[K\n",
            "remote: Compressing objects: 100% (265/265), done.\u001b[K\n",
            "remote: Total 309 (delta 61), reused 275 (delta 36), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (309/309), 5.05 MiB | 16.95 MiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightly"
      ],
      "metadata": {
        "id": "Sq2ETTDiRdjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from lightly.data import ImageCollateFunction, LightlyDataset, collate\n",
        "from lightly.loss import NegativeCosineSimilarity\n",
        "from lightly.models.modules.heads import SimSiamPredictionHead, SimSiamProjectionHead"
      ],
      "metadata": {
        "id": "sqEqKztpfVzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download data\n",
        "!gdown 15C5spf1la7L09kvWXll2qt67Ec0rwLsY\n",
        "# unzip data\n",
        "!tar -zxf aia_171_color_1perMonth.tar.gz && rm aia_171_color_1perMonth.tar.gz"
      ],
      "metadata": {
        "id": "O_QRxTmgj-44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a8f0a8-afb6-4054-89f1-f83d37143fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15C5spf1la7L09kvWXll2qt67Ec0rwLsY\n",
            "To: /content/aia_171_color_1perMonth.tar.gz\n",
            "100% 146M/146M [00:01<00:00, 93.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path_to_data = '/content/gdrive/MyDrive/HITS/aia_171_color_1perMonth/'\n",
        "path_to_data = '/content/aia_171_color_1perMonth'"
      ],
      "metadata": {
        "id": "q_S0omQr2tky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 8 # How many process giving model to train -- similar to threading\n",
        "batch_size = 32 # A subset of files that the model sees to update it's parameters\n",
        "seed = 1 # Seed for random generator for reproducability\n",
        "epochs = 50 # How many times we go through our entire data set\n",
        "input_size = 120 #The number of pixels in x or y\n",
        "\n",
        "# dimension of the embeddings\n",
        "num_ftrs = 512 \n",
        "# dimension of the output of the prediction and projection heads\n",
        "out_dim = proj_hidden_dim = 512\n",
        "# the prediction head uses a bottleneck architecture\n",
        "pred_hidden_dim = 128"
      ],
      "metadata": {
        "id": "PXOSeFsifUSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed torch and numpy \n",
        "# used for reproducibility in creating the model\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "TGu52CY1fk5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the augmentations for self-supervised learning\n",
        "collate_fn = ImageCollateFunction(\n",
        "    input_size=input_size,\n",
        "    # require invariance to flips and rotations\n",
        "    hf_prob=0.5,\n",
        "    vf_prob=0.5,\n",
        "    rr_prob=0.5,\n",
        "    # satellite images are all taken from the same height\n",
        "    # so we use only slight random cropping\n",
        "    min_scale=0.5,\n",
        "    # use a weak color jitter for invariance w.r.t small color changes\n",
        "    cj_prob=0.2,\n",
        "    cj_bright=0.1,\n",
        "    cj_contrast=0.1,\n",
        "    cj_hue=0.1,\n",
        "    cj_sat=0.1,\n",
        ")\n",
        "\n",
        "#test for the collate function\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "img = np.zeros((128,128,3))\n",
        "img = torchvision.transforms.ToPILImage()(np.uint8(255*img))\n",
        "\n",
        "input = [(img, 0, 'my-image.png')]\n",
        "\n",
        "output = collate_fn(input)\n",
        "\n",
        "(img_t0, img_t1), label, filename = output\n",
        "\n",
        "# print(img_t0.shape, img_t1.shape)\n",
        "\n",
        "\n",
        "\n",
        "# create a lightly dataset for training, since the augmentations are handled\n",
        "# by the collate function, there is no need to apply additional ones here\n",
        "dataset_train_simsiam = LightlyDataset(input_dir=path_to_data)\n",
        "\n",
        "#3283 x 32 = 10506\n",
        "print(len(dataset_train_simsiam))\n",
        "# returns image, folder num, tile name\n",
        "print(dataset_train_simsiam[800])\n",
        "\n",
        "\n",
        "# create a dataloader for training\n",
        "dataloader_train_simsiam = torch.utils.data.DataLoader(\n",
        "    dataset_train_simsiam,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,           # data reshuffled at every epoch if True\n",
        "    collate_fn=collate_fn,  # constructs function\n",
        "    drop_last=True,         # If want to merge datasets (optional) - mostly used when batches are loaded from map-styled datasets.\n",
        "    num_workers=num_workers,\n",
        ")\n",
        "\n",
        "# create a torchvision transformation for embedding the dataset after training\n",
        "# here, we resize the images to match the input size during training and apply\n",
        "# a normalization of the color channel based on statistics from imagenet\n",
        "test_transforms = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.Resize((input_size, input_size)),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(\n",
        "            mean=collate.imagenet_normalize[\"mean\"],\n",
        "            std=collate.imagenet_normalize[\"std\"],\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# create a lightly dataset for embedding\n",
        "dataset_test = LightlyDataset(input_dir=path_to_data, transform=test_transforms)\n",
        "\n",
        "# create a dataloader for embedding\n",
        "dataloader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdPjPH4iiQGI",
        "outputId": "9143d7f7-5fa0-4b7a-a1ca-bd69fe29ef3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105056\n",
            "(<PIL.Image.Image image mode=RGB size=128x128 at 0x7FE85715F970>, 1, '20100703_000036_aia.lev1_euv_12s_4k/tiles/20100703_000036_aia.lev1_euv_12s_4k_tile_1024_2944.jpg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimSiam(nn.Module):\n",
        "    def __init__(self, backbone, num_ftrs, proj_hidden_dim, pred_hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.projection_head = SimSiamProjectionHead(num_ftrs, proj_hidden_dim, out_dim)\n",
        "        self.prediction_head = SimSiamPredictionHead(out_dim, pred_hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get representations\n",
        "        f = self.backbone(x).flatten(start_dim=1)\n",
        "        # get projections\n",
        "        z = self.projection_head(f)\n",
        "        # get predictions\n",
        "        p = self.prediction_head(z)\n",
        "        # stop gradient\n",
        "        z = z.detach()\n",
        "        return z, p\n",
        "\n",
        "\n",
        "# we use a pretrained resnet for this tutorial to speed\n",
        "# up training time but you can also train one from scratch\n",
        "resnet = torchvision.models.resnet18()\n",
        "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "model = SimSiam(backbone, num_ftrs, proj_hidden_dim, pred_hidden_dim, out_dim)"
      ],
      "metadata": {
        "id": "gZS_xfsqxOlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SimSiam uses a symmetric negative cosine similarity loss\n",
        "criterion = NegativeCosineSimilarity()\n",
        "\n",
        "# scale the learning rate\n",
        "lr = 0.05 * batch_size / 256\n",
        "# use SGD with momentum and weight decay\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "q1uZbgoPzI12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "avg_loss = 0.0\n",
        "avg_output_std = 0.0\n",
        "for e in range(epochs):\n",
        "    batch_count = 0\n",
        "    for (x0, x1), _, _ in dataloader_train_simsiam:\n",
        "        # move images to the gpu\n",
        "        x0 = x0.to(device)\n",
        "        x1 = x1.to(device)\n",
        "\n",
        "        # run the model on both transforms of the images\n",
        "        # we get projections (z0 and z1) and\n",
        "        # predictions (p0 and p1) as output\n",
        "        z0, p0 = model(x0)\n",
        "        z1, p1 = model(x1)\n",
        "\n",
        "        # apply the symmetric negative cosine similarity\n",
        "        # and run backpropagation\n",
        "        loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # calculate the per-dimension standard deviation of the outputs\n",
        "        # we can use this later to check whether the embeddings are collapsing\n",
        "        output = p0.detach()\n",
        "        output = torch.nn.functional.normalize(output, dim=1)\n",
        "\n",
        "        output_std = torch.std(output, 0)\n",
        "        output_std = output_std.mean()\n",
        "        print(batch_count)\n",
        "        batch_count += 1\n",
        "        print(loss)\n",
        "        if(batch_count == 10):\n",
        "          break\n",
        "        # use moving averages to track the loss and standard deviation\n",
        "        w = 0.9\n",
        "        avg_loss = w * avg_loss + (1 - w) * loss.item()\n",
        "        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\n",
        "\n",
        "    # the level of collapse is large if the standard deviation of the l2\n",
        "    # normalized output is much smaller than 1 / sqrt(dim)\n",
        "    collapse_level = max(0.0, 1 - math.sqrt(out_dim) * avg_output_std)\n",
        "    # print intermediate results\n",
        "    print(\n",
        "        f\"[Epoch {e:3d}] \"\n",
        "        f\"Loss = {avg_loss:.2f} | \"\n",
        "        f\"Collapse Level: {collapse_level:.2f} / 1.00\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pcDaEIek0E_c",
        "outputId": "a6610210-353a-451f-be27-6083d9e893b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "tensor(-0.0182, grad_fn=<MulBackward0>)\n",
            "1\n",
            "tensor(-0.0187, grad_fn=<MulBackward0>)\n",
            "2\n",
            "tensor(-0.0224, grad_fn=<MulBackward0>)\n",
            "3\n",
            "tensor(-0.0387, grad_fn=<MulBackward0>)\n",
            "4\n",
            "tensor(-0.0412, grad_fn=<MulBackward0>)\n",
            "5\n",
            "tensor(-0.0397, grad_fn=<MulBackward0>)\n",
            "6\n",
            "tensor(-0.0404, grad_fn=<MulBackward0>)\n",
            "7\n",
            "tensor(-0.0519, grad_fn=<MulBackward0>)\n",
            "8\n",
            "tensor(-0.0506, grad_fn=<MulBackward0>)\n",
            "9\n",
            "tensor(-0.0369, grad_fn=<MulBackward0>)\n",
            "[Epoch   0] Loss = -0.02 | Collapse Level: 0.50 / 1.00\n",
            "0\n",
            "tensor(-0.0756, grad_fn=<MulBackward0>)\n",
            "1\n",
            "tensor(-0.0771, grad_fn=<MulBackward0>)\n",
            "2\n",
            "tensor(-0.0692, grad_fn=<MulBackward0>)\n",
            "3\n",
            "tensor(-0.0850, grad_fn=<MulBackward0>)\n",
            "4\n",
            "tensor(-0.0870, grad_fn=<MulBackward0>)\n",
            "5\n",
            "tensor(-0.0913, grad_fn=<MulBackward0>)\n",
            "6\n",
            "tensor(-0.1101, grad_fn=<MulBackward0>)\n",
            "7\n",
            "tensor(-0.1122, grad_fn=<MulBackward0>)\n",
            "8\n",
            "tensor(-0.1267, grad_fn=<MulBackward0>)\n",
            "9\n",
            "tensor(-0.1142, grad_fn=<MulBackward0>)\n",
            "[Epoch   1] Loss = -0.07 | Collapse Level: 0.31 / 1.00\n",
            "0\n",
            "tensor(-0.1138, grad_fn=<MulBackward0>)\n",
            "1\n",
            "tensor(-0.1407, grad_fn=<MulBackward0>)\n",
            "2\n",
            "tensor(-0.1202, grad_fn=<MulBackward0>)\n",
            "3\n",
            "tensor(-0.1232, grad_fn=<MulBackward0>)\n",
            "4\n",
            "tensor(-0.1348, grad_fn=<MulBackward0>)\n",
            "5\n",
            "tensor(-0.1511, grad_fn=<MulBackward0>)\n",
            "6\n",
            "tensor(-0.1412, grad_fn=<MulBackward0>)\n",
            "7\n",
            "tensor(-0.1492, grad_fn=<MulBackward0>)\n",
            "8\n",
            "tensor(-0.1406, grad_fn=<MulBackward0>)\n",
            "9\n",
            "tensor(-0.1642, grad_fn=<MulBackward0>)\n",
            "[Epoch   2] Loss = -0.11 | Collapse Level: 0.24 / 1.00\n",
            "0\n",
            "tensor(-0.1784, grad_fn=<MulBackward0>)\n",
            "1\n",
            "tensor(-0.1798, grad_fn=<MulBackward0>)\n",
            "2\n",
            "tensor(-0.1734, grad_fn=<MulBackward0>)\n",
            "3\n",
            "tensor(-0.2121, grad_fn=<MulBackward0>)\n",
            "4\n",
            "tensor(-0.2086, grad_fn=<MulBackward0>)\n",
            "5\n",
            "tensor(-0.2141, grad_fn=<MulBackward0>)\n",
            "6\n",
            "tensor(-0.2082, grad_fn=<MulBackward0>)\n",
            "7\n",
            "tensor(-0.2275, grad_fn=<MulBackward0>)\n",
            "8\n",
            "tensor(-0.1951, grad_fn=<MulBackward0>)\n",
            "9\n",
            "tensor(-0.2041, grad_fn=<MulBackward0>)\n",
            "[Epoch   3] Loss = -0.17 | Collapse Level: 0.22 / 1.00\n",
            "0\n",
            "tensor(-0.2383, grad_fn=<MulBackward0>)\n",
            "1\n",
            "tensor(-0.2500, grad_fn=<MulBackward0>)\n",
            "2\n",
            "tensor(-0.2542, grad_fn=<MulBackward0>)\n",
            "3\n",
            "tensor(-0.2865, grad_fn=<MulBackward0>)\n",
            "4\n",
            "tensor(-0.2342, grad_fn=<MulBackward0>)\n",
            "5\n",
            "tensor(-0.3002, grad_fn=<MulBackward0>)\n",
            "6\n",
            "tensor(-0.3076, grad_fn=<MulBackward0>)\n",
            "7\n",
            "tensor(-0.3161, grad_fn=<MulBackward0>)\n",
            "8\n",
            "tensor(-0.3176, grad_fn=<MulBackward0>)\n",
            "9\n",
            "tensor(-0.2844, grad_fn=<MulBackward0>)\n",
            "[Epoch   4] Loss = -0.24 | Collapse Level: 0.22 / 1.00\n",
            "0\n",
            "tensor(-0.2754, grad_fn=<MulBackward0>)\n",
            "1\n",
            "tensor(-0.3280, grad_fn=<MulBackward0>)\n",
            "2\n",
            "tensor(-0.3361, grad_fn=<MulBackward0>)\n",
            "3\n",
            "tensor(-0.3238, grad_fn=<MulBackward0>)\n",
            "4\n",
            "tensor(-0.3469, grad_fn=<MulBackward0>)\n",
            "5\n",
            "tensor(-0.3406, grad_fn=<MulBackward0>)\n",
            "6\n",
            "tensor(-0.3316, grad_fn=<MulBackward0>)\n",
            "7\n",
            "tensor(-0.3607, grad_fn=<MulBackward0>)\n",
            "8\n",
            "tensor(-0.3513, grad_fn=<MulBackward0>)\n",
            "9\n",
            "tensor(-0.4187, grad_fn=<MulBackward0>)\n",
            "[Epoch   5] Loss = -0.30 | Collapse Level: 0.22 / 1.00\n",
            "0\n",
            "tensor(-0.3208, grad_fn=<MulBackward0>)\n",
            "1\n",
            "tensor(-0.3527, grad_fn=<MulBackward0>)\n",
            "2\n",
            "tensor(-0.3976, grad_fn=<MulBackward0>)\n",
            "3\n",
            "tensor(-0.4011, grad_fn=<MulBackward0>)\n",
            "4\n",
            "tensor(-0.3614, grad_fn=<MulBackward0>)\n",
            "5\n",
            "tensor(-0.3713, grad_fn=<MulBackward0>)\n",
            "6\n",
            "tensor(-0.4406, grad_fn=<MulBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-bebd5d62ae27>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# and run backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd hits-sdo-similaritysearch/search_simsiam/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVVpPfYB4mIJ",
        "outputId": "03bbb836-6eb7-40a7-c995-7e1066e9245c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hits-sdo-similaritysearch/search_simsiam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout ss_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fs17BfM4s6-",
        "outputId": "f8dd4525-9bec-4974-cb33-93ba87fbb36f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'ss_dataloader' set up to track remote branch 'ss_dataloader' from 'origin'.\n",
            "Switched to a new branch 'ss_dataloader'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm that branch is up to date\n",
        "!git log --oneline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdnaiW8342DD",
        "outputId": "5a90708c-5550-453b-dca7-076719dcb5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33me47b776\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mss_dataloader\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/ss_dataloader\u001b[m\u001b[33m)\u001b[m team-sunbird used stitch_adj_images to fill voids in two augmented views\n",
            "\u001b[33macdcd3c\u001b[m\u001b[33m (\u001b[m\u001b[1;31morigin/main\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m, \u001b[m\u001b[1;32mmain\u001b[m\u001b[33m)\u001b[m Merge pull request #1 from hits-sdo/initialization\n",
            "\u001b[33mb6b76a7\u001b[m\u001b[33m (\u001b[m\u001b[1;31morigin/initialization\u001b[m\u001b[33m)\u001b[m Make main the default branch in the notebook setup\n",
            "\u001b[33m3e41ea2\u001b[m Added download to setup notebook\n",
            "\u001b[33m7135c8a\u001b[m replaced 'search-utils' with 'search_utils'\n",
            "\u001b[33m8c2014c\u001b[m replaced '-' with '_' in module names\n",
            "\u001b[33m97a1002\u001b[m added search_utils to sys.path\n",
            "\u001b[33mdd9c215\u001b[m added search-utils to sys.path\n",
            "\u001b[33m0702056\u001b[m renamed folder module utils to search-utils\n",
            "\u001b[33m68990e6\u001b[m renamed folder module utils to search-utils\n",
            "\u001b[33m23f7e1e\u001b[m renamed folder module utils to search-utils\n",
            "\u001b[33mcae0aef\u001b[m renamed folder module utils to search-utils\n",
            "\u001b[33m341a9e8\u001b[m two augmentations from same source image Co-authored-by: Jasper <jasperdoan@users.noreply.github.com> Co-authored-by: max9001 <max9001@users.noreply.github.com> Co-authored-by: Matin Qurbanzadeh <MatinQurban@users.noreply.github.com> Co-authored-by: ThomasP677 <ThomasP677@users.noreply.github.com> Co-authored-by: Cameron Wolff <camwolff02@users.noreply.github.com> Co-authored-by: agomez225 <agomez225@users.noreply.github.com>\n",
            "\u001b[33m290033d\u001b[m two different augmentations from same source image\n",
            "\u001b[33m49b7753\u001b[m Merge branch 'initialization' of https://github.com/hits-sdo/hits-sdo-similaritysearch into initialization\n",
            "\u001b[33m1561b9f\u001b[m Removed unnecessary commits in notebook\n",
            "\u001b[33m4c65ec8\u001b[m Merge branch 'initialization' of https://github.com/hits-sdo/hits-sdo-similaritysearch into initialization\n",
            "\u001b[33m90e272f\u001b[m corrected bug in file length calculation\n",
            "\u001b[33md357801\u001b[m converted utils folder to module\n",
            "\u001b[33m4c97358\u001b[m moved to utils folder\n",
            "\u001b[33m3d9abed\u001b[m Wrote the setup for dependencies in requirements.txt\n",
            "\u001b[33m060fc49\u001b[m Merge branch 'initialization' of https://github.com/hits-sdo/hits-sdo-similaritysearch into initialization\n",
            "\u001b[33m2c267cd\u001b[m added image reader and super image generator\n",
            "\u001b[33mb523fa6\u001b[m Merge branch 'initialization' of https://github.com/hits-sdo/hits-sdo-similaritysearch into initialization\n",
            "\u001b[33mc88d5fe\u001b[m Add intialization Notebook\n",
            "\u001b[33mb07d5d7\u001b[m added notebook to demonstrate the entire experiment\n",
            "\u001b[33m86e9956\u001b[m added pickle files of monochrome AIA171 tiles\n",
            "\u001b[33m2703ad8\u001b[m added opencv and matplotlib\n",
            "\u001b[33m0232721\u001b[m added opencv and matplotlib\n",
            "\u001b[33m87cda42\u001b[m Add conda environment and pip requirement files\n",
            "\u001b[33m25d2b32\u001b[m convert folders to modules\n",
            "\u001b[33m3fcfe3a\u001b[m convert simsiam folder to module\n",
            "\u001b[33m753e232\u001b[m Add augmentation dictionary file\n",
            "\u001b[33m5244ae1\u001b[m Initial commit\n"
          ]
        }
      ]
    }
  ]
}