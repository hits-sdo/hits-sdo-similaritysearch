{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TODO:\n",
        "\n",
        "- change `fill_type = 'Nearest'` to super image\n",
        "  - nearest fill type generates artificial edge\n",
        "  - will using neigbhoring images to form a super image (non artifical) make a difference?\n",
        "\n",
        "- develop training and (validate and/or test) sets that are distinct\n",
        "  - test set must be labeled for us to evalute model. \n",
        "    - training and validate can be unlabled. we can evaluate based on clusters.\n",
        "\n",
        "- run entire (training) set in colab eventually...?\n",
        "  - 80 20 split\n"
      ],
      "metadata": {
        "id": "UWmLbVHi4gxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch and Download data"
      ],
      "metadata": {
        "id": "bYmKe5rSjxIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 15C5spf1la7L09kvWXll2qt67Ec0rwLsY                                          # Download data\n",
        "!tar -zxf aia_171_color_1perMonth.tar.gz && rm aia_171_color_1perMonth.tar.gz     # Unzip data"
      ],
      "metadata": {
        "id": "O_QRxTmgj-44",
        "outputId": "9b09322e-4c52-4891-9006-8454e75ec927",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15C5spf1la7L09kvWXll2qt67Ec0rwLsY\n",
            "To: /content/aia_171_color_1perMonth.tar.gz\n",
            "100% 146M/146M [00:00<00:00, 187MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clones repo & Install all libraries and requirements"
      ],
      "metadata": {
        "id": "0_iBrFqGj0gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hits-sdo/hits-sdo-similaritysearch                  # Clone repo"
      ],
      "metadata": {
        "id": "xpKcrCT_4Myq",
        "outputId": "7b2d7b03-0762-410b-cff8-caaa379d62c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'hits-sdo-similaritysearch' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move repo's directory as workspace\n",
        "%cd hits-sdo-similaritysearch/"
      ],
      "metadata": {
        "id": "rD5MAwycqTV0",
        "outputId": "331e9570-68c2-4a79-a5be-4a06bebc3619",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hits-sdo-similaritysearch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# switch to correct branch\n",
        "!git checkout ss_dataloader"
      ],
      "metadata": {
        "id": "npnwtclT0DvG",
        "outputId": "c7184015-9239-4c57-94e1-69d32d9277c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already on 'ss_dataloader'\n",
            "Your branch is up to date with 'origin/ss_dataloader'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -r requirements.txt                                                  # Install all libraries\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "DdDc4olnqcaE",
        "outputId": "bcc1f237-4424-4f7d-92ef-84cb8a7886fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/hits-sdo/hits-sdo-packager.git@pip_nodata (from -r requirements.txt (line 11))\n",
            "  Cloning https://github.com/hits-sdo/hits-sdo-packager.git (to revision pip_nodata) to /tmp/pip-req-build-mp0l4r4u\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/hits-sdo/hits-sdo-packager.git /tmp/pip-req-build-mp0l4r4u\n",
            "  Running command git checkout -b pip_nodata --track origin/pip_nodata\n",
            "  Switched to a new branch 'pip_nodata'\n",
            "  Branch 'pip_nodata' set up to track remote branch 'pip_nodata' from 'origin'.\n",
            "  Resolved https://github.com/hits-sdo/hits-sdo-packager.git to commit 3a54caba2ae6bf7caf4eabaf580a6ffda1b3bbda\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lightning (from -r requirements.txt (line 6))\n",
            "  Downloading lightning-2.0.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightly (from -r requirements.txt (line 7))\n",
            "  Downloading lightly-1.4.7-py3-none-any.whl (647 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.6/647.6 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sunpy (from -r requirements.txt (line 8))\n",
            "  Downloading sunpy-4.1.7-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.5.3)\n",
            "Collecting wandb (from -r requirements.txt (line 10))\n",
            "  Downloading wandb-0.15.4-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2<5.0 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (3.1.2)\n",
            "Requirement already satisfied: PyYAML<8.0 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (6.0)\n",
            "Collecting arrow<3.0,>=1.2.0 (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (4.11.2)\n",
            "Requirement already satisfied: click<10.0 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (8.1.3)\n",
            "Collecting croniter<1.4.0,>=1.3.0 (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading croniter-1.3.15-py2.py3-none-any.whl (19 kB)\n",
            "Collecting dateutils<2.0 (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading dateutils-0.6.12-py2.py3-none-any.whl (5.7 kB)\n",
            "Collecting deepdiff<8.0,>=5.7.0 (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading deepdiff-6.3.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi<0.89.0,>=0.69.0 (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading fastapi-0.88.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec<2024.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (2023.4.0)\n",
            "Collecting inquirer<5.0,>=2.10.0 (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading inquirer-3.1.3-py3-none-any.whl (18 kB)\n",
            "Collecting lightning-cloud>=0.5.34 (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading lightning_cloud-0.5.36-py3-none-any.whl (562 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m562.4/562.4 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities<2.0,>=0.7.0 (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (23.1)\n",
            "Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (5.9.5)\n",
            "Requirement already satisfied: pydantic<4.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (1.10.7)\n",
            "Collecting python-multipart<2.0,>=0.0.5 (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (2.27.1)\n",
            "Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (13.3.4)\n",
            "Collecting starlette (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading starlette-0.28.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starsessions<2.0,>=1.2.1 (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: torch<4.0,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (2.0.1+cu118)\n",
            "Collecting torchmetrics<2.0,>=0.7.0 (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (4.65.0)\n",
            "Requirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (5.7.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (1.26.15)\n",
            "Collecting uvicorn<2.0 (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning->-r requirements.txt (line 6)) (1.5.1)\n",
            "Collecting websockets<12.0 (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading pytorch_lightning-2.0.3-py3-none-any.whl (720 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.6/720.6 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from lightly->-r requirements.txt (line 7)) (2022.12.7)\n",
            "Collecting hydra-core>=1.0.0 (from lightly->-r requirements.txt (line 7))\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightly-utils~=0.0.0 (from lightly->-r requirements.txt (line 7))\n",
            "  Downloading lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from lightly->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from lightly->-r requirements.txt (line 7)) (1.16.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from lightly->-r requirements.txt (line 7)) (0.15.2+cu118)\n",
            "Requirement already satisfied: astropy>=4.2.1 in /usr/local/lib/python3.10/dist-packages (from sunpy->-r requirements.txt (line 8)) (5.2.2)\n",
            "Collecting parfive[ftp]>=1.2.0 (from sunpy->-r requirements.txt (line 8))\n",
            "  Downloading parfive-2.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 9)) (2022.7.1)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 10))\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 10))\n",
            "  Downloading sentry_sdk-1.25.1-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.7/206.7 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 10))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb->-r requirements.txt (line 10))\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb->-r requirements.txt (line 10))\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 10)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 10)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 10)) (3.20.3)\n",
            "Requirement already satisfied: pyerfa>=2.0 in /usr/local/lib/python3.10/dist-packages (from astropy>=4.2.1->sunpy->-r requirements.txt (line 8)) (2.0.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning->-r requirements.txt (line 6)) (2.4.1)\n",
            "Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff<8.0,>=5.7.0->lightning->-r requirements.txt (line 6))\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting starlette (from lightning->-r requirements.txt (line 6))\n",
            "  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->lightning->-r requirements.txt (line 6)) (3.6.2)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec<2024.0,>=2022.5.0->lightning->-r requirements.txt (line 6))\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 10))\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.4,>=2.2 (from hydra-core>=1.0.0->lightly->-r requirements.txt (line 7))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.0.0->lightly->-r requirements.txt (line 7))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting blessed>=1.19.0 (from inquirer<5.0,>=2.10.0->lightning->-r requirements.txt (line 6))\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-editor>=1.0.4 (from inquirer<5.0,>=2.10.0->lightning->-r requirements.txt (line 6))\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting readchar>=3.0.6 (from inquirer<5.0,>=2.10.0->lightning->-r requirements.txt (line 6))\n",
            "  Downloading readchar-4.0.5-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<5.0->lightning->-r requirements.txt (line 6)) (2.1.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from lightly-utils~=0.0.0->lightly->-r requirements.txt (line 7)) (8.4.0)\n",
            "Collecting pyjwt (from lightning-cloud>=0.5.34->lightning->-r requirements.txt (line 6))\n",
            "  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n",
            "Collecting aioftp>=0.17.1 (from parfive[ftp]>=1.2.0->sunpy->-r requirements.txt (line 8))\n",
            "  Downloading aioftp-0.21.4-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning->-r requirements.txt (line 6)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning->-r requirements.txt (line 6)) (3.4)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning->-r requirements.txt (line 6)) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning->-r requirements.txt (line 6)) (2.14.0)\n",
            "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from starsessions<2.0,>=1.2.1->lightning->-r requirements.txt (line 6)) (2.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning->-r requirements.txt (line 6)) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning->-r requirements.txt (line 6)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning->-r requirements.txt (line 6)) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning->-r requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<4.0,>=1.11.0->lightning->-r requirements.txt (line 6)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<4.0,>=1.11.0->lightning->-r requirements.txt (line 6)) (16.0.5)\n",
            "Collecting h11>=0.8 (from uvicorn<2.0->lightning->-r requirements.txt (line 6))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning->-r requirements.txt (line 6)) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning->-r requirements.txt (line 6))\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning->-r requirements.txt (line 6))\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning->-r requirements.txt (line 6))\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning->-r requirements.txt (line 6))\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning->-r requirements.txt (line 6))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->lightning->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning->-r requirements.txt (line 6)) (0.2.6)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 10))\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<15.0,>=12.3.0->lightning->-r requirements.txt (line 6)) (0.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.11.0->lightning->-r requirements.txt (line 6)) (1.3.0)\n",
            "Building wheels for collected packages: hits-sdo-packager, antlr4-python3-runtime, pathtools\n",
            "  Building wheel for hits-sdo-packager (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hits-sdo-packager: filename=hits_sdo_packager-0.1.0-py3-none-any.whl size=11251 sha256=36cc4bb83ebcf67abd9419c57c992b485cf7ce6bd37692811cf5c818dabb961f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fhdm33tx/wheels/2e/eb/6a/bd29fa0f13c312509ef59785fdf5297327b72cf53190878e91\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=1212a91c3d285a45693280737023605320de6802eab35e378b64aa20e508e1f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=2d3600e5eab1c58c37df4fc7656067a83f5e368add257a8bf0048f47637ba8c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built hits-sdo-packager antlr4-python3-runtime pathtools\n",
            "Installing collected packages: python-editor, pathtools, antlr4-python3-runtime, websockets, smmap, setproctitle, sentry-sdk, readchar, python-multipart, pyjwt, ordered-set, omegaconf, multidict, lightning-utilities, lightly-utils, hits-sdo-packager, h11, frozenlist, docker-pycreds, blessed, async-timeout, aioftp, yarl, uvicorn, starlette, inquirer, hydra-core, gitdb, deepdiff, dateutils, croniter, arrow, aiosignal, starsessions, GitPython, fastapi, aiohttp, wandb, parfive, lightning-cloud, sunpy, torchmetrics, pytorch-lightning, lightning, lightly\n",
            "Successfully installed GitPython-3.1.31 aioftp-0.21.4 aiohttp-3.8.4 aiosignal-1.3.1 antlr4-python3-runtime-4.9.3 arrow-1.2.3 async-timeout-4.0.2 blessed-1.20.0 croniter-1.3.15 dateutils-0.6.12 deepdiff-6.3.0 docker-pycreds-0.4.0 fastapi-0.88.0 frozenlist-1.3.3 gitdb-4.0.10 h11-0.14.0 hits-sdo-packager-0.1.0 hydra-core-1.3.2 inquirer-3.1.3 lightly-1.4.7 lightly-utils-0.0.2 lightning-2.0.3 lightning-cloud-0.5.36 lightning-utilities-0.8.0 multidict-6.0.4 omegaconf-2.3.0 ordered-set-4.1.0 parfive-2.0.2 pathtools-0.1.2 pyjwt-2.7.0 python-editor-1.0.4 python-multipart-0.0.6 pytorch-lightning-2.0.3 readchar-4.0.5 sentry-sdk-1.25.1 setproctitle-1.3.2 smmap-5.0.0 starlette-0.22.0 starsessions-1.3.0 sunpy-4.1.7 torchmetrics-0.11.4 uvicorn-0.22.0 wandb-0.15.4 websockets-11.0.3 yarl-1.9.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.25.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import packages / modules"
      ],
      "metadata": {
        "id": "yCDJ-QTBk-ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from lightly.data import ImageCollateFunction, LightlyDataset, collate\n",
        "from lightly.loss import NegativeCosineSimilarity\n",
        "from lightly.models.modules.heads import ( \n",
        "    SimSiamPredictionHead, \n",
        "    SimSiamProjectionHead\n",
        ")\n",
        "\n",
        "import os\n",
        "import matplotlib.offsetbox as osb                                            \n",
        "\n",
        "import torchvision.transforms.functional as functional\n",
        "from matplotlib import rcParams as rcp\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn import random_projection\n",
        "\n",
        "from sdo_augmentation.augmentation_list import AugmentationList\n",
        "from sdo_augmentation.augmentation import Augmentations\n",
        "\n",
        "\n",
        "\n",
        "from search_utils.image_utils import read_image, stitch_adj_imgs\n",
        "from search_simsiam.custom_collate import sunbirdCollate\n",
        "\n",
        "import glob"
      ],
      "metadata": {
        "id": "sqEqKztpfVzo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up path to raw data"
      ],
      "metadata": {
        "id": "Yl-QMbEImC0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_data = '/content/aia_171_color_1perMonth'                                 # Path to raw data"
      ],
      "metadata": {
        "id": "Hmn1aN_JmA8u"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Hyper-parameters"
      ],
      "metadata": {
        "id": "RVcCgqE2mFDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 2                                                                   # How many process giving model to train -- similar to threading\n",
        "batch_size = 1024                                                                 # A subset of files that the model sees to update it's parameters\n",
        "seed = 1                                                                          # Seed for random generator for reproducability\n",
        "epochs = 20                                                                       # How many times we go through our entire data set\n",
        "input_size = 128                                                                  # The number of pixels in x or y\n",
        "\n",
        "num_ftrs = 512                                                                    # Dimension of the embeddings\n",
        "\n",
        "\n",
        "# Dimension of the output of the prediction and projection heads\n",
        "out_dim = proj_hidden_dim = 512\n",
        "\n",
        "# The prediction head uses a bottleneck architecture\n",
        "pred_hidden_dim = 128"
      ],
      "metadata": {
        "id": "PXOSeFsifUSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fix seed for reproducibility"
      ],
      "metadata": {
        "id": "nhUg7zQ4tdCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seed torch and numpy \n",
        "# used for reproducibility in creating the model\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "TGu52CY1fk5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining dataset and data loader\n",
        "\n",
        "*   Setting up data set \n",
        "*   Build the collete function \n",
        "*   List item\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h0k2DhuE09_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# create a lightly dataset for training, since the augmentations are handled\n",
        "# by the collate function, there is no need to apply additional ones here\n",
        "dataset_train_simsiam = LightlyDataset(input_dir=path_to_data)\n",
        "\n",
        "#3283 x 32 = 10506\n",
        "print(len(dataset_train_simsiam))\n",
        "# returns image, folder num, tile name\n",
        "print(dataset_train_simsiam[800])"
      ],
      "metadata": {
        "id": "xxMCmBio3qOx",
        "outputId": "4889c32e-d6a4-4647-fc3a-d68796a11a23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105056\n",
            "(<PIL.Image.Image image mode=RGB size=128x128 at 0x7F88A3417C40>, 1, '20100703_000036_aia.lev1_euv_12s_4k/tiles/20100703_000036_aia.lev1_euv_12s_4k_tile_1024_2944.jpg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building collate function "
      ],
      "metadata": {
        "id": "Yfr2fAj44-CT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#achieve similar function as the top using the fil_type\n",
        "#an example wold be a nested function \n",
        "#implement through a class or a nested function \n",
        "\n",
        "fill_type = \"SuperImage\"\n",
        "sun_bird_collate_fn = sunbirdCollate(fill_type=fill_type)"
      ],
      "metadata": {
        "id": "CNPpUnFN3RcI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_simsiam[0]\n",
        "print(dataset_train_simsiam[10])\n",
        "(img_t0,img_t1),label,file_name = sun_bird_collate_fn([dataset_train_simsiam[10]])\n",
        "\n",
        "plt.figure(figsize = (10,10))\n",
        "\n",
        "print(label,file_name)\n",
        "plt.subplot(2,2,1)\n",
        "plt.imshow(img_t0.permute(2,3,1,0)[:,:,:,0])\n",
        "plt.ylabel(\"Team Sunbird's Collate Function\")\n",
        "plt.subplot(2,2,2)\n",
        "plt.imshow(img_t1.permute(2,3,1,0)[:,:,:,0])\n",
        "\n",
        "\n",
        "dataset_train_simsiam[0]\n",
        "(img_t0,img_t1),label,file_name = collate_fn([dataset_train_simsiam[10]])\n",
        "\n",
        "print(label,file_name)\n",
        "plt.subplot(2,2,3)\n",
        "plt.imshow(img_t0.permute(2,3,1,0)[:,:,:,0])\n",
        "plt.ylabel(\"Default Collate Function\")\n",
        "plt.subplot(2,2,4)\n",
        "plt.imshow(img_t1.permute(2,3,1,0)[:,:,:,0])"
      ],
      "metadata": {
        "id": "eTm0bEX7YrT6",
        "outputId": "2eb1c169-db0c-4db6-e1e5-c6ff68d6581f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<PIL.Image.Image image mode=RGB size=128x128 at 0x7F889C26FE50>, 0, '20100601_000036_aia.lev1_euv_12s_4k/tiles/20100601_000036_aia.lev1_euv_12s_4k_tile_1024_2304.jpg')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-a388140d8065>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_train_simsiam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train_simsiam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mimg_t0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_t1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msun_bird_collate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_train_simsiam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/hits-sdo-similaritysearch/hits-sdo-similaritysearch/search_simsiam/custom_collate.py\u001b[0m in \u001b[0;36msunbird_collate\u001b[0;34m(source_tuple)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mpath_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mfile_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_data\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mfiles_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/*.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'path_to_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(img_t0.shape)\n",
        "something = torch.from_numpy(np.array(img_t0)).unsqueeze(0).permute(0,3,1,2)\n",
        "print(something.shape)"
      ],
      "metadata": {
        "id": "letBMmD4bRlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#test for the collate function\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "input = [(dataset_train_simsiam[800][0], 0, 'my-image.png')]\n",
        "\n",
        "output = collate_fn(input)\n",
        "\n",
        "(img_t0, img_t1), label, filename = output\n",
        "\n",
        "print(img_t0.shape, img_t1.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "bQgeMA5GGOBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimSiam(nn.Module):\n",
        "    def __init__(self, backbone, num_ftrs, proj_hidden_dim, pred_hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.projection_head = SimSiamProjectionHead(num_ftrs, proj_hidden_dim, out_dim)\n",
        "        self.prediction_head = SimSiamPredictionHead(out_dim, pred_hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get representations\n",
        "        f = self.backbone(x).flatten(start_dim=1)\n",
        "        # get projections\n",
        "        z = self.projection_head(f)\n",
        "        # get predictions\n",
        "        p = self.prediction_head(z)\n",
        "        # stop gradient\n",
        "        z = z.detach()\n",
        "        return z, p\n",
        "\n",
        "\n",
        "# we use a pretrained resnet for this tutorial to speed\n",
        "# up training time but you can also train one from scratch\n",
        "resnet = torchvision.models.resnet18()\n",
        "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "model = SimSiam(backbone, num_ftrs, proj_hidden_dim, pred_hidden_dim, out_dim)"
      ],
      "metadata": {
        "id": "gZS_xfsqxOlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SimSiam uses a symmetric negative cosine similarity loss\n",
        "criterion = NegativeCosineSimilarity()\n",
        "\n",
        "# scale the learning rate\n",
        "lr = 0.05 * batch_size / 256\n",
        "# use SGD with momentum and weight decay\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "q1uZbgoPzI12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import random # for demo script\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "vK1zZHHqXgMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(\n",
        "    entity=\"search-byol\",\n",
        "    # Set the project where this run will be logged\n",
        "    project=\"search-simsiam\",\n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "        \"learning_rate\": lr,\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"collate_fn\": \"sunbird_collate\",\n",
        "        \"fill_type\": fill_type,\n",
        "    })\n"
      ],
      "metadata": {
        "id": "DVdWxGEXXtwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#true if currently have GPU\n",
        "torch.cuda.is_available()\n",
        "\n",
        "#check pgu count\n",
        "#not number of workers, that is seperate\n",
        "torch.cuda.device_count()"
      ],
      "metadata": {
        "id": "YMAH2ODzFUp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "device = \"mps\" if torch.backends.mps.is_available() else device\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "loss_list = []\n",
        "collapse_list = []\n",
        "\n",
        "avg_loss = 0.0\n",
        "avg_output_std = 0.0\n",
        "for e in range(epochs):\n",
        "    batch_count = 0\n",
        "\n",
        "    # for each batch in dataloader\n",
        "    # x0: first augmentation batch\n",
        "    # x1: second augmentation batch\n",
        "    for (x0, x1), _, _ in dataloader_train_simsiam:\n",
        "        # move images to the gpu\n",
        "        x0 = x0.float().to(device)\n",
        "        x1 = x1.float().to(device)\n",
        "\n",
        "        # print shape of batches\n",
        "        print(f'Batch shapes: {x0.shape}, {x1.shape}')\n",
        "\n",
        "        # run the model on both transforms of the images\n",
        "        # we get projections (z0 and z1) and\n",
        "        # predictions (p0 and p1) as output\n",
        "        z0, p0 = model(x0)\n",
        "        z1, p1 = model(x1)\n",
        "\n",
        "        # apply the symmetric negative cosine similarity\n",
        "        # and run backpropagation\n",
        "        loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # calculate the per-dimension standard deviation of the outputs\n",
        "        # we can use this later to check whether the embeddings are collapsing\n",
        "        output = p0.detach()\n",
        "        output = torch.nn.functional.normalize(output, dim=1)\n",
        "\n",
        "        output_std = torch.std(output, 0)\n",
        "        output_std = output_std.mean()\n",
        "        batch_count += 1\n",
        "        print(batch_count)\n",
        "        print(loss)\n",
        "        # if(batch_count == 10):\n",
        "        #   break\n",
        "        # use moving averages to track the loss and standard deviation\n",
        "        w = 0.9\n",
        "        avg_loss = w * avg_loss + (1 - w) * loss.item()\n",
        "        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\n",
        "\n",
        "    # the level of collapse is large if the standard deviation of the l2\n",
        "    # normalized output is much smaller than 1 / sqrt(dim)\n",
        "    collapse_level = max(0.0, 1 - math.sqrt(out_dim) * avg_output_std)\n",
        "    # print intermediate results\n",
        "    print(\n",
        "        f\"[Epoch {e:3d}] \"\n",
        "        f\"Loss = {avg_loss:.2f} | \"\n",
        "        f\"Collapse Level: {collapse_level:.2f} / 1.00\"\n",
        "    )\n",
        "\n",
        "    loss_list.append(avg_loss)\n",
        "    collapse_list.append(collapse_level)\n",
        "    wandb.log({\"simsiam_loss\": avg_loss, \"simsiam_collapse_level\": collapse_level})"
      ],
      "metadata": {
        "id": "pcDaEIek0E_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot epochs vs lost\n",
        "plt.plot(loss_list, label='Loss')\n",
        "#plt.plot(collapse_list, label='Collapse')\n",
        "plt.legend(frameon=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GfA6vZHOME5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot epochs vs lost\n",
        "#plt.plot(loss_list, label='Loss')\n",
        "plt.plot(collapse_list, label='Collapse')\n",
        "plt.legend(frameon=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xrnAvWfR2YEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that the model is trained, embed images into dataset\n",
        "embeddings = []\n",
        "filenames = []\n",
        "\n",
        "# disable gradients for faster calculations\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # passes batches and filenames to model to find embeddings\n",
        "    # embedding -> vectorize image, simpler representation of image\n",
        "    for i, (x, _, fnames) in enumerate(dataloader_test):\n",
        "        # move the images to the gpu\n",
        "        x = x.to(device)\n",
        "        # embed the images with the pre-trained backbone\n",
        "        y = model.backbone(x).flatten(start_dim=1)\n",
        "        # store the embeddings and filenames in lists\n",
        "        embeddings.append(y)\n",
        "        filenames = filenames + list(fnames)\n",
        "\n",
        "# concatenate the embeddings and convert to numpy\n",
        "embeddings = torch.cat(embeddings, dim=0)\n",
        "embeddings = embeddings.cpu().numpy()"
      ],
      "metadata": {
        "id": "VBEvtkfsPIAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform embeddings to 2D using UMAP (dimension reduction algorithm)\n",
        "\n",
        "# for the scatter plot we want to transform the images to a two-dimensional\n",
        "# vector space using a random Gaussian projection\n",
        "projection = random_projection.GaussianRandomProjection(n_components=2)\n",
        "embeddings_2d = projection.fit_transform(embeddings)\n",
        "\n",
        "# normalize the embeddings to fit in the [0, 1] square\n",
        "M = np.max(embeddings_2d, axis=0)\n",
        "m = np.min(embeddings_2d, axis=0)\n",
        "embeddings_2d = (embeddings_2d - m) / (M - m)"
      ],
      "metadata": {
        "id": "rVXr-CLZQa5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display a scatter plot of the dataset\n",
        "# clustering similar images together\n",
        "\n",
        "def get_scatter_plot_with_thumbnails():\n",
        "    \"\"\"Creates a scatter plot with image overlays.\"\"\"\n",
        "    # initialize empty figure and add subplot\n",
        "    fig = plt.figure()\n",
        "    fig.suptitle(\"Scatter Plot of the SDO/AIA 171 Tiles\")\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    # shuffle images and find out which images to show\n",
        "    shown_images_idx = []\n",
        "    shown_images = np.array([[1.0, 1.0]])\n",
        "    iterator = [i for i in range(embeddings_2d.shape[0])]\n",
        "    np.random.shuffle(iterator)\n",
        "    for i in iterator:\n",
        "        # only show image if it is sufficiently far away from the others\n",
        "        dist = np.sum((embeddings_2d[i] - shown_images) ** 2, 1)\n",
        "        if np.min(dist) < 2e-3:\n",
        "            continue\n",
        "        shown_images = np.r_[shown_images, [embeddings_2d[i]]]\n",
        "        shown_images_idx.append(i)\n",
        "\n",
        "    # plot image overlays\n",
        "    for idx in shown_images_idx:\n",
        "        thumbnail_size = int(rcp[\"figure.figsize\"][0] * 2.0)\n",
        "        path = os.path.join(path_to_data, filenames[idx])\n",
        "        img = Image.open(path)\n",
        "        img = functional.resize(img, thumbnail_size)\n",
        "        img = np.array(img)\n",
        "        img_box = osb.AnnotationBbox(\n",
        "            osb.OffsetImage(img, cmap=plt.cm.gray_r),\n",
        "            embeddings_2d[idx],\n",
        "            pad=0.2,\n",
        "        )\n",
        "        ax.add_artist(img_box)\n",
        "\n",
        "    # set aspect ratio\n",
        "    ratio = 1.0 / ax.get_data_ratio()\n",
        "    ax.set_aspect(ratio, adjustable=\"box\")\n",
        "\n",
        "\n",
        "# get a scatter plot with thumbnail overlays\n",
        "get_scatter_plot_with_thumbnails()"
      ],
      "metadata": {
        "id": "jgxejiOMRMBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes the 8 closest images to each example image and plot them together\n",
        "\n",
        "example_images = [filenames[10**n] for n in range(5)]\n",
        "\n",
        "def get_image_as_np_array(filename: str):\n",
        "    \"\"\"Loads the image with filename and returns it as a numpy array.\"\"\"\n",
        "    img = Image.open(filename)\n",
        "    return np.asarray(img)\n",
        "\n",
        "\n",
        "def get_image_as_np_array_with_frame(filename: str, w: int = 5):\n",
        "    \"\"\"Returns an image as a numpy array with a black frame of width w.\"\"\"\n",
        "    img = get_image_as_np_array(filename)\n",
        "    ny, nx, _ = img.shape\n",
        "    # create an empty image with padding for the frame\n",
        "    framed_img = np.zeros((w + ny + w, w + nx + w, 3))\n",
        "    framed_img = framed_img.astype(np.uint8)\n",
        "    # put the original image in the middle of the new one\n",
        "    framed_img[w:-w, w:-w] = img\n",
        "    return framed_img\n",
        "\n",
        "\n",
        "def plot_nearest_neighbors_3x3(example_image: str, i: int):\n",
        "    \"\"\"Plots the example image and its eight nearest neighbors.\"\"\"\n",
        "    n_subplots = 9\n",
        "    # initialize empty figure\n",
        "    fig = plt.figure()\n",
        "    fig.suptitle(f\"Nearest Neighbor Plot {i + 1}\")\n",
        "    #\n",
        "    example_idx = filenames.index(example_image)\n",
        "    # get distances to the cluster center\n",
        "    distances = embeddings - embeddings[example_idx]\n",
        "    distances = np.power(distances, 2).sum(-1).squeeze()\n",
        "    # sort indices by distance to the center\n",
        "    nearest_neighbors = np.argsort(distances)[:n_subplots]\n",
        "    # show images\n",
        "    for plot_offset, plot_idx in enumerate(nearest_neighbors):\n",
        "        ax = fig.add_subplot(3, 3, plot_offset + 1)\n",
        "        # get the corresponding filename\n",
        "        fname = os.path.join(path_to_data, filenames[plot_idx])\n",
        "        if plot_offset == 0:\n",
        "            ax.set_title(f\"Query Image\")\n",
        "            plt.imshow(get_image_as_np_array_with_frame(fname))\n",
        "        else:\n",
        "            plt.imshow(get_image_as_np_array(fname))\n",
        "        # let's disable the axis\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "\n",
        "# show example images for each cluster\n",
        "for i, example_image in enumerate(example_images):\n",
        "    plot_nearest_neighbors_3x3(example_image, i)"
      ],
      "metadata": {
        "id": "_BD6Qi5IRphf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}