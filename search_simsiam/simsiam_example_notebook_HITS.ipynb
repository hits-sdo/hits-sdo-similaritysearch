{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hits-sdo/hits-sdo-similaritysearch/blob/ss_dataloader/search_simsiam/simsiam_example_notebook_HITS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hits-sdo/hits-sdo-similaritysearch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpKcrCT_4Myq",
        "outputId": "6b6d072f-f84c-4b65-c415-1655560d9229"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hits-sdo-similaritysearch'...\n",
            "remote: Enumerating objects: 319, done.\u001b[K\n",
            "remote: Counting objects: 100% (319/319), done.\u001b[K\n",
            "remote: Compressing objects: 100% (273/273), done.\u001b[K\n",
            "remote: Total 319 (delta 66), reused 278 (delta 38), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (319/319), 5.06 MiB | 17.20 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd hits-sdo-similaritysearch/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm_gS85P9qv6",
        "outputId": "b5af8b1a-58a9-4609-b6b0-caa29b299161"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hits-sdo-similaritysearch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "0ma6EWaJ9dxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from lightly.data import ImageCollateFunction, LightlyDataset, collate\n",
        "from lightly.loss import NegativeCosineSimilarity\n",
        "from lightly.models.modules.heads import SimSiamPredictionHead, SimSiamProjectionHead"
      ],
      "metadata": {
        "id": "sqEqKztpfVzo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download data\n",
        "!gdown 15C5spf1la7L09kvWXll2qt67Ec0rwLsY\n",
        "# unzip data\n",
        "!tar -zxf aia_171_color_1perMonth.tar.gz && rm aia_171_color_1perMonth.tar.gz"
      ],
      "metadata": {
        "id": "O_QRxTmgj-44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590c8c1b-79fb-49b7-bd60-9d15f1142770"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15C5spf1la7L09kvWXll2qt67Ec0rwLsY\n",
            "To: /content/aia_171_color_1perMonth.tar.gz\n",
            "100% 146M/146M [00:01<00:00, 130MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path_to_data = '/content/gdrive/MyDrive/HITS/aia_171_color_1perMonth/'\n",
        "path_to_data = '/content/aia_171_color_1perMonth'"
      ],
      "metadata": {
        "id": "q_S0omQr2tky"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 8 # How many process giving model to train -- similar to threading\n",
        "batch_size = 32 # A subset of files that the model sees to update it's parameters\n",
        "seed = 1 # Seed for random generator for reproducability\n",
        "epochs = 50 # How many times we go through our entire data set\n",
        "input_size = 128 #The number of pixels in x or y\n",
        "\n",
        "# dimension of the embeddings\n",
        "num_ftrs = 512 \n",
        "# dimension of the output of the prediction and projection heads\n",
        "out_dim = proj_hidden_dim = 512\n",
        "# the prediction head uses a bottleneck architecture\n",
        "pred_hidden_dim = 128"
      ],
      "metadata": {
        "id": "PXOSeFsifUSL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed torch and numpy \n",
        "# used for reproducibility in creating the model\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "TGu52CY1fk5q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the augmentations for self-supervised learning\n",
        "collate_fn = ImageCollateFunction(\n",
        "    input_size=input_size,\n",
        "    # require invariance to flips and rotations\n",
        "    hf_prob=0.5,\n",
        "    vf_prob=0.5,\n",
        "    rr_prob=0.5,\n",
        "    # satellite images are all taken from the same height\n",
        "    # so we use only slight random cropping\n",
        "    min_scale=0.5,\n",
        "    # use a weak color jitter for invariance w.r.t small color changes\n",
        "    cj_prob=0.2,\n",
        "    cj_bright=0.1,\n",
        "    cj_contrast=0.1,\n",
        "    cj_hue=0.1,\n",
        "    cj_sat=0.1,\n",
        ")\n",
        "\n",
        "#test for the collate function\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "img = np.zeros((128,128,3))\n",
        "img = torchvision.transforms.ToPILImage()(np.uint8(255*img))\n",
        "\n",
        "input = [(img, 0, 'my-image.png')]\n",
        "\n",
        "output = collate_fn(input)\n",
        "\n",
        "(img_t0, img_t1), label, filename = output\n",
        "\n",
        "# print(img_t0.shape, img_t1.shape)\n",
        "\n",
        "\n",
        "\n",
        "# create a lightly dataset for training, since the augmentations are handled\n",
        "# by the collate function, there is no need to apply additional ones here\n",
        "dataset_train_simsiam = LightlyDataset(input_dir=path_to_data)\n",
        "\n",
        "#3283 x 32 = 10506\n",
        "print(len(dataset_train_simsiam))\n",
        "# returns image, folder num, tile name\n",
        "print(dataset_train_simsiam[800])\n",
        "\n",
        "\n",
        "# create a dataloader for training\n",
        "dataloader_train_simsiam = torch.utils.data.DataLoader(\n",
        "    dataset_train_simsiam,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,           # data reshuffled at every epoch if True\n",
        "    collate_fn=collate_fn,  # constructs function\n",
        "    drop_last=True,         # If want to merge datasets (optional) - mostly used when batches are loaded from map-styled datasets.\n",
        "    num_workers=num_workers,\n",
        ")\n",
        "\n",
        "# create a torchvision transformation for embedding the dataset after training\n",
        "# here, we resize the images to match the input size during training and apply\n",
        "# a normalization of the color channel based on statistics from imagenet\n",
        "test_transforms = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.Resize((input_size, input_size)),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(\n",
        "            mean=collate.imagenet_normalize[\"mean\"],\n",
        "            std=collate.imagenet_normalize[\"std\"],\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# create a lightly dataset for embedding\n",
        "dataset_test = LightlyDataset(input_dir=path_to_data, transform=test_transforms)\n",
        "\n",
        "# create a dataloader for embedding\n",
        "dataloader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdPjPH4iiQGI",
        "outputId": "5c7e7ddd-7be2-4ad1-9c8c-25ab5d90b769"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105056\n",
            "(<PIL.Image.Image image mode=RGB size=128x128 at 0x7FCE44FFC8E0>, 1, '20100703_000036_aia.lev1_euv_12s_4k/tiles/20100703_000036_aia.lev1_euv_12s_4k_tile_1024_2944.jpg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimSiam(nn.Module):\n",
        "    def __init__(self, backbone, num_ftrs, proj_hidden_dim, pred_hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.projection_head = SimSiamProjectionHead(num_ftrs, proj_hidden_dim, out_dim)\n",
        "        self.prediction_head = SimSiamPredictionHead(out_dim, pred_hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get representations\n",
        "        f = self.backbone(x).flatten(start_dim=1)\n",
        "        # get projections\n",
        "        z = self.projection_head(f)\n",
        "        # get predictions\n",
        "        p = self.prediction_head(z)\n",
        "        # stop gradient\n",
        "        z = z.detach()\n",
        "        return z, p\n",
        "\n",
        "\n",
        "# we use a pretrained resnet for this tutorial to speed\n",
        "# up training time but you can also train one from scratch\n",
        "resnet = torchvision.models.resnet18()\n",
        "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "model = SimSiam(backbone, num_ftrs, proj_hidden_dim, pred_hidden_dim, out_dim)"
      ],
      "metadata": {
        "id": "gZS_xfsqxOlC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SimSiam uses a symmetric negative cosine similarity loss\n",
        "criterion = NegativeCosineSimilarity()\n",
        "\n",
        "# scale the learning rate\n",
        "lr = 0.05 * batch_size / 256\n",
        "# use SGD with momentum and weight decay\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "q1uZbgoPzI12"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "avg_loss = 0.0\n",
        "avg_output_std = 0.0\n",
        "for e in range(epochs):\n",
        "    batch_count = 0\n",
        "    for (x0, x1), _, _ in dataloader_train_simsiam:\n",
        "        # move images to the gpu\n",
        "        x0 = x0.to(device)\n",
        "        x1 = x1.to(device)\n",
        "\n",
        "        # run the model on both transforms of the images\n",
        "        # we get projections (z0 and z1) and\n",
        "        # predictions (p0 and p1) as output\n",
        "        z0, p0 = model(x0)\n",
        "        z1, p1 = model(x1)\n",
        "\n",
        "        # apply the symmetric negative cosine similarity\n",
        "        # and run backpropagation\n",
        "        loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # calculate the per-dimension standard deviation of the outputs\n",
        "        # we can use this later to check whether the embeddings are collapsing\n",
        "        output = p0.detach()\n",
        "        output = torch.nn.functional.normalize(output, dim=1)\n",
        "\n",
        "        output_std = torch.std(output, 0)\n",
        "        output_std = output_std.mean()\n",
        "        print(batch_count)\n",
        "        batch_count += 1\n",
        "        print(loss)\n",
        "        if(batch_count == 10):\n",
        "          break\n",
        "        # use moving averages to track the loss and standard deviation\n",
        "        w = 0.9\n",
        "        avg_loss = w * avg_loss + (1 - w) * loss.item()\n",
        "        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\n",
        "\n",
        "    # the level of collapse is large if the standard deviation of the l2\n",
        "    # normalized output is much smaller than 1 / sqrt(dim)\n",
        "    collapse_level = max(0.0, 1 - math.sqrt(out_dim) * avg_output_std)\n",
        "    # print intermediate results\n",
        "    print(\n",
        "        f\"[Epoch {e:3d}] \"\n",
        "        f\"Loss = {avg_loss:.2f} | \"\n",
        "        f\"Collapse Level: {collapse_level:.2f} / 1.00\"\n",
        "    )"
      ],
      "metadata": {
        "id": "pcDaEIek0E_c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}