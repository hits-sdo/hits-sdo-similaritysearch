{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hits-sdo/hits-sdo-similaritysearch/blob/byol_index/search_byol/byol_train_pl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSmPQhQA8cPS"
      },
      "source": [
        "# Notebook to Intialize HITS-SDO self-similarity search environment\n",
        "- Run all cells to initalize environment, and restart runtime if prompted to use updated versions. You will need to rerun the cells again to ensure that all dependencies have been installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqLSpbNoO1po"
      },
      "source": [
        "# Download and Unzip Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erAMHVo9NS7U",
        "outputId": "3a433f91-9b24-4ef6-b554-7ca8c5e80517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DMIatOmA4XcoWeW0oAUkZujx8YrhLkpY\n",
            "To: /content/AIA211_193_171_Miniset.tar.gz\n",
            "100% 121M/121M [00:02<00:00, 41.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-BPQpdQWNwZzoVpg3lQ143cr3_UPE36M\n",
            "To: /content/AIA211_193_171_Miniset_ext.tar.gz\n",
            "100% 175M/175M [00:04<00:00, 43.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download Data - Uncomment the one you want\n",
        "\n",
        "# 171 with false yellow colors - https://drive.google.com/file/d/15C5spf1la7L09kvWXll2qt67Ec0rwLsY/view?usp=drive_link\n",
        "# !gdown 15C5spf1la7L09kvWXll2qt67Ec0rwLsY\n",
        "\n",
        "\n",
        "# 171 grayscale - https://drive.google.com/file/d/16WD0td1f5gx4yIIDkWWSTb-oZcezI1CU/view?usp=drive_link\n",
        "# !gdown 16WD0td1f5gx4yIIDkWWSTb-oZcezI1CU\n",
        "\n",
        "\n",
        "# Multiwavelength r:211 g:193 b:171 https://drive.google.com/file/d/1DMIatOmA4XcoWeW0oAUkZujx8YrhLkpY/view?usp=sharing\n",
        "!gdown 1DMIatOmA4XcoWeW0oAUkZujx8YrhLkpY\n",
        "# Multiwavelength r:211 g:193 b:171active cycle extension https://drive.google.com/file/d/1-BPQpdQWNwZzoVpg3lQ143cr3_UPE36M/view?usp=sharing\n",
        "!gdown 1-BPQpdQWNwZzoVpg3lQ143cr3_UPE36M\n",
        "\n",
        "# Multiwavelength r:304 g:211 b:171 https://drive.google.com/file/d/1Pts431S-fdSfJJ6pt79-fkNYazXqO9O7/view?usp=sharing\n",
        "# !gdown 1Pts431S-fdSfJJ6pt79-fkNYazXqO9O7\n",
        "# Multiwavelength r:304 g:211 b:171 active cycle extension https://drive.google.com/file/d/1Sbbgi6HzJkuCTUF8tJTLeddpGIjGXTcj/view?usp=sharing\n",
        "# !gdown 1Sbbgi6HzJkuCTUF8tJTLeddpGIjGXTcj\n",
        "\n",
        "\n",
        "# Multiwavelength r:335 g:193 b:94 https://drive.google.com/file/d/1v0IYSzwiQcHPcnSsTAeU7bI8c4310b35/view?usp=sharing\n",
        "# !gdown 1v0IYSzwiQcHPcnSsTAeU7bI8c4310b35\n",
        "# Multiwavelength r:335 g:193 b:94 active cycle extension https://drive.google.com/file/d/1uSt8BzpVj5GgM9SwSsDcQMEE4lv1uTJo/view?usp=sharing\n",
        "# !gdown 1uSt8BzpVj5GgM9SwSsDcQMEE4lv1uTJo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJYUU6FAOJeM"
      },
      "outputs": [],
      "source": [
        "# Unzip file make them match the cell above\n",
        "\n",
        "# 171 with false yellow colors\n",
        "# !tar -zxf aia_171_color_1perMonth.tar.gz\n",
        "\n",
        "\n",
        "# 171 grayscale\n",
        "# !tar -zxf AIA171_Miniset_BW.tar.gz\n",
        "\n",
        "\n",
        "# Multiwavelength r:211 g:193 b:171\n",
        "!tar -zxf AIA211_193_171_Miniset.tar.gz\n",
        "# Multiwavelength r:211 g:193 b:171active cycle extension\n",
        "!tar -zxf AIA211_193_171_Miniset_ext.tar.gz\n",
        "\n",
        "\n",
        "# Multiwavelength r:304 g:211 b:171\n",
        "# !tar -zxf AIA304_211_171_Miniset.tar.gz\n",
        "# Multiwavelength r:304 g:211 b:171 active cycle extension\n",
        "# !tar -zxf AIA304_211_171_Miniset_ext.tar.gz\n",
        "\n",
        "\n",
        "# Multiwavelength r:335 g:193 b:94\n",
        "# !tar -zxf AIA335_193_94_Miniset.tar.gz\n",
        "# Multiwavelength r:335 g:193 b:94 active cycle extension\n",
        "# !tar -zxf AIA335_193_94_Miniset_ext.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYwjORgaP2yY"
      },
      "outputs": [],
      "source": [
        "# Print some files to see that they exist - match cells above\n",
        "\n",
        "# 171 with false yellow colors\n",
        "# !du aia_171_color_1perMonth/. -l -h\n",
        "\n",
        "# 171 grayscale\n",
        "# !du AIA171_Miniset_BW/. -l -h\n",
        "\n",
        "# Multiwavelength r:211 g:193 b:171\n",
        "# !du AIA211_193_171_Miniset/. -l -h\n",
        "\n",
        "# Multiwavelength r:304 g:211 b:171\n",
        "# !du AIA304_211_171_Miniset/. -l -h\n",
        "\n",
        "# Multiwavelength r:335 g:193 b:94\n",
        "# !du AIA335_193_94_Miniset/. -l -h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gNxeWeSQRUp"
      },
      "source": [
        "# Clone repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blhqiMyQ8f8c",
        "outputId": "ebf78b56-f1e4-4ffb-9827-be536d4117aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'hits-sdo-similaritysearch'...\n",
            "remote: Enumerating objects: 1361, done.\u001b[K\n",
            "remote: Counting objects: 100% (587/587), done.\u001b[K\n",
            "remote: Compressing objects: 100% (312/312), done.\u001b[K\n",
            "remote: Total 1361 (delta 326), reused 449 (delta 262), pack-reused 774\u001b[K\n",
            "Receiving objects: 100% (1361/1361), 217.96 MiB | 17.09 MiB/s, done.\n",
            "Resolving deltas: 100% (688/688), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository from GitHub\n",
        "!git clone https://github.com/hits-sdo/hits-sdo-similaritysearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1gZD1xt-uoz",
        "outputId": "c62c1511-dbbd-4df6-8c3c-e59d7c5dbe1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/hits-sdo-similaritysearch\n"
          ]
        }
      ],
      "source": [
        "%cd hits-sdo-similaritysearch/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZQt3zDtQf1G"
      },
      "source": [
        "# Switch to Desired Branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_Lr1XLG931E",
        "outputId": "e39c0ec4-64f8-4e6d-9b5a-bbd8823f05a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Branch 'byol_losses' set up to track remote branch 'byol_losses' from 'origin'.\n",
            "Switched to a new branch 'byol_losses'\n"
          ]
        }
      ],
      "source": [
        "# Switch to the desired branch with requirements.txt\n",
        "!git checkout byol_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AWACo_j-emk"
      },
      "outputs": [],
      "source": [
        "# Confirm that branch is up to date\n",
        "# !git log --oneline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r65HUQl7QoSM"
      },
      "source": [
        "# Install all necesary packages into environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3AWVW-w9RSV",
        "outputId": "7e4d6930-e9cd-4e85-dd26-0b7a442a47d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m633.0/633.0 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.7/596.7 kB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.8/728.8 kB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.4/722.4 kB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.2/213.2 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hits-sdo-packager (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EQmyquSd49S"
      },
      "source": [
        "##  Load Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niefkB7Pd49S",
        "outputId": "5079f4e6-5a8b-411f-80aa-609fee84bf57"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.chdir('/home/amunoz/hits-sdo-similaritysearch')\n",
        "\n",
        "import copy\n",
        "import hdbscan as hdbscan_cpu\n",
        "from cuml.cluster import hdbscan\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.colors as colors\n",
        "import matplotlib\n",
        "\n",
        "from lightly.data import LightlyDataset\n",
        "from lightly.data.multi_view_collate import MultiViewCollate\n",
        "from lightly.loss import NegativeCosineSimilarity, NTXentLoss\n",
        "from lightly.models.modules import BYOLPredictionHead, BYOLProjectionHead\n",
        "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
        "from lightly.transforms.simclr_transform import SimCLRTransform\n",
        "from lightly.utils.scheduler import cosine_schedule\n",
        "\n",
        "import seaborn as sns\n",
        "from search_byol.dataset import SDOTilesDataset\n",
        "from search_utils.image_utils import read_image\n",
        "import umap\n",
        "\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "from cuml.metrics.cluster.silhouette_score import cython_silhouette_score as silhouette_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AeUD0yxW5-h"
      },
      "source": [
        "## Login to Wanddb and initialize logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_Xla6gJX8Lh",
        "outputId": "97c0a796-1090-4966-b77a-8d3ec373dd74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/amunoz/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login --relogin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZuoKah_dWpe"
      },
      "source": [
        "## Define run parameters and initalize Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Car-gZLeMUiW"
      },
      "outputs": [],
      "source": [
        "# Set data type and data path.  Make sure it matches cells above\n",
        "\n",
        "# 171 with false yellow colors\n",
        "# data_type = '171Color'\n",
        "# data_path = '/content/aia_171_color_1perMonth'\n",
        "\n",
        "# 171 grayscale\n",
        "# data_type = '171gray'\n",
        "# data_path = '/content/AIA171_Miniset_BW'\n",
        "\n",
        "# Multiwavelength r:211 g:193 b:171\n",
        "# data_type = 'r:211 g:193 b:171'\n",
        "data_type = 'r:211 g:193 b:171 extended'\n",
        "data_path = '/content/AIA211_193_171_Miniset'\n",
        "# data_path = '/mnt/d/Mis Documentos/AAResearch/SEARCH/hits-sdo-downloader/AIA211_193_171_Miniset_exp/AIA211_193_171_Miniset'\n",
        "\n",
        "# Multiwavelength r:304 g:211 b:171\n",
        "# data_type = 'r:304 g:211 b:171'\n",
        "# data_type = 'r:304 g:211 b:171 extended'\n",
        "# data_path = '/content/AIA304_211_171_Miniset'\n",
        "\n",
        "# Multiwavelength r:335 g:193 b:94\n",
        "# data_type = 'r:335 g:193 b:94'\n",
        "# data_type = 'r:335 g:193 b:94 extended'\n",
        "# data_path = '/content/AIA335_193_94_Miniset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hEDgOBn4dfyn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamunozj\u001b[0m (\u001b[33msearch-byol\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20230714_135112-joqc3253</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/search-byol/search-byol-for_real/runs/joqc3253' target=\"_blank\">AMJ-ds15_bs512_lr0.1_doubleaug_ss0.1_se1.0_pjs32_pds32_contrast</a></strong> to <a href='https://wandb.ai/search-byol/search-byol-for_real' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/search-byol/search-byol-for_real' target=\"_blank\">https://wandb.ai/search-byol/search-byol-for_real</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/search-byol/search-byol-for_real/runs/joqc3253' target=\"_blank\">https://wandb.ai/search-byol/search-byol-for_real/runs/joqc3253</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "seed = 42 # So clever.\n",
        "pl.seed_everything(seed, workers=True)\n",
        "\n",
        "initial = \"AMJ\"\n",
        "job_type = \"HDB-SCAN-Sweep2\"\n",
        "epochs = 4\n",
        "data_stride = 15\n",
        "batch_size = 512\n",
        "augmentation = 'double'\n",
        "loss = 'contrast'   # 'contrast' or 'cos'\n",
        "learning_rate = 0.1\n",
        "cosine_scheduler_start = .1\n",
        "cosine_scheduler_end = 1.0\n",
        "projection_size = 32\n",
        "prediction_size = 32\n",
        "\n",
        "notes = \"\"\n",
        "name = f\"{initial}-ds{data_stride}_bs{batch_size}_lr{learning_rate}_{augmentation}aug_ss{cosine_scheduler_start}_se{cosine_scheduler_end}_pjs{projection_size}_pds{prediction_size}_{loss}\"\n",
        "group = \"clustering\"\n",
        "tags = [\"experimentation\"]\n",
        "\n",
        "\n",
        "wandb_logger = WandbLogger(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"search-byol-for_real\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"seed\": seed,\n",
        "    \"data_type\": data_type,\n",
        "    \"batch size\": batch_size,\n",
        "    \"augmention\": augmentation,\n",
        "    \"data stride\": data_stride,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"cosine_scheduler_start\": cosine_scheduler_start,\n",
        "    \"cosine_scheduler_end\": cosine_scheduler_end\n",
        "    },\n",
        "    entity = \"search-byol\",\n",
        "    job_type = job_type,\n",
        "    name = name,\n",
        "    notes = notes,\n",
        "    group = group,\n",
        "    tags = tags,\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTTNr9Qad49S"
      },
      "source": [
        "## Define BYOL Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gl8JlRFfd49S"
      },
      "outputs": [],
      "source": [
        "class BYOL(pl.LightningModule):\n",
        "    def __init__(self, lr=0.1, projection_size=256, prediction_size=256, cosine_scheduler_start=0.1, cosine_scheduler_end=1.0, epochs=10, loss='cos'):\n",
        "        super().__init__()\n",
        "\n",
        "        resnet = torchvision.models.resnet18() # Play w/ resnet.\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.projection_head = BYOLProjectionHead(512, 1024, projection_size)\n",
        "        self.prediction_head = BYOLPredictionHead(projection_size, 1024, prediction_size)\n",
        "\n",
        "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
        "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
        "\n",
        "        deactivate_requires_grad(self.backbone_momentum)\n",
        "        deactivate_requires_grad(self.projection_head_momentum)\n",
        "\n",
        "\n",
        "        self.loss = loss\n",
        "        self.loss_cos = NegativeCosineSimilarity()\n",
        "        self.loss_contrast = NTXentLoss()\n",
        "\n",
        "        self.cosine_scheduler_start = cosine_scheduler_start\n",
        "        self.cosine_scheduler_end = cosine_scheduler_end\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.backbone(x).flatten(start_dim=1)\n",
        "        z = self.projection_head(y)\n",
        "        p = self.prediction_head(z)\n",
        "        return p\n",
        "\n",
        "    def forward_momentum(self, x):\n",
        "        y = self.backbone_momentum(x).flatten(start_dim=1)\n",
        "        z = self.projection_head_momentum(y)\n",
        "        z = z.detach()\n",
        "        return z\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "\n",
        "        momentum = cosine_schedule(self.current_epoch, self.epochs, self.cosine_scheduler_start, self.cosine_scheduler_end)\n",
        "        update_momentum(self.backbone, self.backbone_momentum, m=momentum)\n",
        "        update_momentum(self.projection_head, self.projection_head_momentum, m=momentum)\n",
        "        (x0, x1, _) = batch\n",
        "        p0 = self.forward(x0)\n",
        "        z0 = self.forward_momentum(x0)\n",
        "        p1 = self.forward(x1)\n",
        "        z1 = self.forward_momentum(x1)\n",
        "\n",
        "        loss_cos = 0.5 * (self.loss_cos(p0, z1) + self.loss_cos(p1, z0))\n",
        "        loss_contrast = 0.5 * (self.loss_contrast(p0, z1) + self.loss_contrast(p1, z0))\n",
        "\n",
        "        if self.loss == 'cos':\n",
        "            loss = loss_cos\n",
        "        else:\n",
        "            loss = loss_contrast\n",
        "\n",
        "        self.log('loss cos', loss_cos)\n",
        "        self.log('loss contrast', loss_contrast)\n",
        "        self.log('loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SGD(self.parameters(), lr=self.lr) # Play w/ optimizers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOaS3xWrd49T"
      },
      "source": [
        "## Initialize Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-4tn1MpKd49T"
      },
      "outputs": [],
      "source": [
        "model = BYOL(lr=learning_rate, projection_size=projection_size, prediction_size=prediction_size, cosine_scheduler_start=cosine_scheduler_start, cosine_scheduler_end=cosine_scheduler_end, loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh9kGF4Ad49T"
      },
      "source": [
        "## Initialize Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HEryt3YUd49T"
      },
      "outputs": [],
      "source": [
        "dataset = SDOTilesDataset(data_path=data_path, augmentation=augmentation, data_stride=data_stride)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        "    num_workers=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oaOqFlRd49U"
      },
      "source": [
        "## Run Training Loop, Inference, and log in wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RSMdSelud49U"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name                     | Type                     | Params\n",
            "----------------------------------------------------------------------\n",
            "0 | backbone                 | Sequential               | 11.2 M\n",
            "1 | projection_head          | BYOLProjectionHead       | 559 K \n",
            "2 | prediction_head          | BYOLPredictionHead       | 67.6 K\n",
            "3 | backbone_momentum        | Sequential               | 11.2 M\n",
            "4 | projection_head_momentum | BYOLProjectionHead       | 559 K \n",
            "5 | loss_cos                 | NegativeCosineSimilarity | 0     \n",
            "6 | loss_contrast            | NTXentLoss               | 0     \n",
            "----------------------------------------------------------------------\n",
            "11.8 M    Trainable params\n",
            "11.7 M    Non-trainable params\n",
            "23.5 M    Total params\n",
            "94.156    Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8df0eb285cc419e9386a4c4c0c30fbe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(max_epochs=epochs,\n",
        "                     accelerator=\"auto\", devices=\"auto\", strategy=\"auto\",\n",
        "                     logger=wandb_logger, log_every_n_steps=10, deterministic=True)\n",
        "\n",
        "wandb_logger.watch(model)\n",
        "trainer.fit(model=model, train_dataloaders=dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGx0ROM5QBdF"
      },
      "source": [
        "### Save model to wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz6ik6KSLQ7L"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), f'{name}_.pt')\n",
        "# wandb.save(f'{name}_.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUcoNAuvQGAK"
      },
      "source": [
        "## Run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4Jj9mNEbOth2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bc3fc61b1234c95a4d1b27630555cba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running inference on batches:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset_inference = SDOTilesDataset(data_path=data_path, augmentation='none', data_stride=5)\n",
        "\n",
        "dataloader_inference = torch.utils.data.DataLoader(\n",
        "    dataset_inference,\n",
        "    batch_size=1000,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        "    num_workers=8,\n",
        ")\n",
        "\n",
        "batches_bar = tqdm(dataloader_inference, dynamic_ncols=True, leave=True, desc=f'Running inference on batches')\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "inference_list = []\n",
        "filename_list = []\n",
        "\n",
        "for (x0, image_file) in batches_bar:\n",
        "    embeddings_inference = model.forward_momentum(x0.to(device)).cpu().numpy()\n",
        "    inference_list.append(embeddings_inference)\n",
        "    filename_list.append(image_file)\n",
        "\n",
        "embeddings_inference = np.concatenate(inference_list, axis=0)\n",
        "returned_images = np.concatenate(filename_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_Z4KFgzkW6n"
      },
      "source": [
        "# Cluster evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Relative validity calculation taken from HDBSCAN cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relative_validity(labels_, minimum_spanning_tree_):\n",
        "\n",
        "    labels = labels_\n",
        "    sizes = np.bincount(labels + 1)\n",
        "    noise_size = sizes[0]\n",
        "    cluster_size = sizes[1:]\n",
        "    total = noise_size + np.sum(cluster_size)\n",
        "    num_clusters = len(cluster_size)\n",
        "    DSC = np.zeros(num_clusters)\n",
        "    min_outlier_sep = np.inf  # only required if num_clusters = 1\n",
        "    correction_const = 2  # only required if num_clusters = 1\n",
        "\n",
        "    # Unltimately, for each Ci, we only require the\n",
        "    # minimum of DSPC(Ci, Cj) over all Cj != Ci.\n",
        "    # So let's call this value DSPC_wrt(Ci), i.e.\n",
        "    # density separation 'with respect to' Ci.\n",
        "    DSPC_wrt = np.ones(num_clusters) * np.inf\n",
        "    max_distance = 0\n",
        "\n",
        "    mst_df = minimum_spanning_tree_.to_pandas()\n",
        "\n",
        "    for edge in mst_df.iterrows():\n",
        "        label1 = labels[int(edge[1][\"from\"])]\n",
        "        label2 = labels[int(edge[1][\"to\"])]\n",
        "        length = edge[1][\"distance\"]\n",
        "\n",
        "        max_distance = max(max_distance, length)\n",
        "\n",
        "        if label1 == -1 and label2 == -1:\n",
        "            continue\n",
        "        elif label1 == -1 or label2 == -1:\n",
        "            # If exactly one of the points is noise\n",
        "            min_outlier_sep = min(min_outlier_sep, length)\n",
        "            continue\n",
        "\n",
        "        if label1 == label2:\n",
        "            # Set the density sparseness of the cluster\n",
        "            # to the sparsest value seen so far.\n",
        "            DSC[label1] = max(length, DSC[label1])\n",
        "        else:\n",
        "            # Check whether density separations with\n",
        "            # respect to each of these clusters can\n",
        "            # be reduced.\n",
        "            DSPC_wrt[label1] = min(length, DSPC_wrt[label1])\n",
        "            DSPC_wrt[label2] = min(length, DSPC_wrt[label2])\n",
        "\n",
        "    # In case min_outlier_sep is still np.inf, we assign a new value to it.\n",
        "    # This only makes sense if num_clusters = 1 since it has turned out\n",
        "    # that the MR-MST has no edges between a noise point and a core point.\n",
        "    min_outlier_sep = max_distance if min_outlier_sep == np.inf else min_outlier_sep\n",
        "\n",
        "    # DSPC_wrt[Ci] might be infinite if the connected component for Ci is\n",
        "    # an \"island\" in the MR-MST. Whereas for other clusters Cj and Ck, the\n",
        "    # MR-MST might contain an edge with one point in Cj and ther other one\n",
        "    # in Ck. Here, we replace the infinite density separation of Ci by\n",
        "    # another large enough value.\n",
        "    #\n",
        "    # TODO: Think of a better yet efficient way to handle this.\n",
        "    correction = correction_const * (\n",
        "        max_distance if num_clusters > 1 else min_outlier_sep\n",
        "    )\n",
        "    DSPC_wrt[np.where(DSPC_wrt == np.inf)] = correction\n",
        "\n",
        "    V_index = [\n",
        "        (DSPC_wrt[i] - DSC[i]) / max(DSPC_wrt[i], DSC[i])\n",
        "        for i in range(num_clusters)\n",
        "    ]\n",
        "    score = np.sum(\n",
        "        [(cluster_size[i] * V_index[i]) / total for i in range(num_clusters)]\n",
        "    )\n",
        "    _relative_validity = score\n",
        "    return _relative_validity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "fFq6WpJOgwji"
      },
      "outputs": [],
      "source": [
        "# specify parameters and distributions to sample from\n",
        "param_dist = {'min_samples': [2, 4, 8, 16, 32],\n",
        "              'min_cluster_size':[2, 4, 8, 16, 32],\n",
        "              'cluster_selection_method' : ['eom','leaf'],\n",
        "              'cluster_selection_epsilon' : [0.0, 0.1, 0.15, 0.2],\n",
        "              'metric' : ['euclidean'],\n",
        "              'gen_min_span_tree' : [True]\n",
        "             }\n",
        "\n",
        "wandb.config.update(param_dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "0mHiZwW4Lt1l"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca6c40df57c3492b8221746aebbb8315",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Best relative validity -1, Best DBindex -1, Best silhouette -1:   0%|          | 0/200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "parameter_list =  list(ParameterGrid(param_dist))\n",
        "relative_validity_list = []\n",
        "DBIndex_list = []\n",
        "silhouette_list = []\n",
        "\n",
        "best_validity=-1\n",
        "best_dbindex=-1\n",
        "best_silhouette=-1\n",
        "parameters_bar = tqdm(ParameterGrid(param_dist), desc=f'Best relative validity {best_validity:>02}, Best DBindex {best_dbindex:>02}, Best silhouette {best_silhouette:>02}')\n",
        "for parameters in parameters_bar:\n",
        "  clusterer = hdbscan.HDBSCAN(**parameters)\n",
        "  clusterer.fit(embeddings_inference)\n",
        "  relative_validity_list.append(relative_validity(clusterer.labels_, clusterer.minimum_spanning_tree_))\n",
        "\n",
        "  ss = silhouette_score(embeddings_inference, clusterer.labels_, metric='cosine')\n",
        "  silhouette_list.append(ss)\n",
        "\n",
        "  dbs = davies_bouldin_score(embeddings_inference, clusterer.labels_)\n",
        "  DBIndex_list.append(dbs)\n",
        "\n",
        "  best_validity = np.max(np.array(relative_validity_list))\n",
        "  best_dbindex = np.min(np.array(DBIndex_list))\n",
        "  best_silhouette= np.max(np.array(silhouette_list))\n",
        "\n",
        "  parameters_bar.set_description(f'Best relative validity {best_validity:>02}, Best DBindex {best_dbindex:>02}, Best silhouette {best_silhouette:>02}')\n",
        "  parameters_bar.refresh()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_umap(data, colors, n_neighbors=15, min_dist=0.1, spread=1, n_components=2, repulsion_strength=1.0, metric='euclidean', title='', alpha=0.1):\n",
        "    fit = umap.UMAP(\n",
        "        n_neighbors=n_neighbors,\n",
        "        min_dist=min_dist,\n",
        "        n_components=n_components,\n",
        "        metric=metric,\n",
        "        spread=spread,\n",
        "        repulsion_strength=repulsion_strength,\n",
        "        verbose=True\n",
        "    )\n",
        "    u = fit.fit_transform(data);\n",
        "    fig = plt.figure(figsize=(16,9), dpi=150)\n",
        "    if n_components == 1:\n",
        "        ax = fig.add_subplot(111)\n",
        "        ax.scatter(u[:,0], range(len(u)), c=colors, s=1, alpha=alpha)\n",
        "    if n_components == 2:\n",
        "        ax = fig.add_subplot(111)\n",
        "        ax.scatter(u[:,0], u[:,1], c=colors, s=1, alpha=alpha)\n",
        "    if n_components == 3:\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.scatter(u[:,0], u[:,1], u[:,2], c=colors, s=1, alpha=alpha)\n",
        "    plt.title(title, fontsize=18)\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvUF1TH_VCpq"
      },
      "outputs": [],
      "source": [
        "best_index = (np.array(relative_validity_list) == np.max(np.array(relative_validity_list))).nonzero()[0][0]\n",
        "best_validity = np.max(np.array(relative_validity_list))\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN(**parameter_list[best_index])\n",
        "clusterer.fit(embeddings_inference)\n",
        "\n",
        "# Plotting best results\n",
        "sns.color_palette('Paired', clusterer.labels_.max()+1)\n",
        "\n",
        "color_palette = sns.color_palette('Paired', clusterer.labels_.max()+1)\n",
        "cluster_colors = [color_palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)]\n",
        "cluster_alphas = np.ones_like(clusterer.labels_)*0.5\n",
        "cluster_alphas[clusterer.labels_==-1] = 0.1\n",
        "\n",
        "fig = draw_umap(data=embeddings_inference, colors=cluster_member_colors, n_neighbors=30, min_dist=0.1, title='n_neighbors = {}'.format(15), alpha=cluster_alphas)\n",
        "\n",
        "wandb.log({\"UMAP 2d Optimal validity\": wandb.Image(fig)})\n",
        "\n",
        "log_df = pd.DataFrame(parameter_list[best_index],index=[0])\n",
        "best_parameters_table = wandb.Table(dataframe=log_df)\n",
        "wandb.log({\"Best validity parameters\": best_parameters_table})\n",
        "\n",
        "parameter_list[best_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td9U56_lh-7n"
      },
      "outputs": [],
      "source": [
        "best_index = (np.array(DBIndex_list) == np.min(np.array(DBIndex_list))).nonzero()[0][0]\n",
        "best_dbindex = np.min(np.array(DBIndex_list))\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN(**parameter_list[best_index])\n",
        "clusterer.fit(embeddings_inference)\n",
        "\n",
        "# Plotting best results\n",
        "sns.color_palette('Paired', clusterer.labels_.max()+1)\n",
        "\n",
        "color_palette = sns.color_palette('Paired', clusterer.labels_.max()+1)\n",
        "cluster_colors = [color_palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)]\n",
        "cluster_alphas = np.ones_like(clusterer.labels_)*0.5\n",
        "cluster_alphas[clusterer.labels_==-1] = 0.1\n",
        "\n",
        "fig = draw_umap(data=embeddings_inference, colors=cluster_member_colors, n_neighbors=30, min_dist=0.1, title='n_neighbors = {}'.format(15), alpha=cluster_alphas)\n",
        "\n",
        "wandb.log({\"UMAP 2d Optimal DBIndex\": wandb.Image(fig)})\n",
        "\n",
        "log_df = pd.DataFrame(parameter_list[best_index],index=[0])\n",
        "best_parameters_table = wandb.Table(dataframe=log_df)\n",
        "wandb.log({\"Best DBIndex parameters\": best_parameters_table})\n",
        "\n",
        "parameter_list[best_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HViNFj9ziZiZ"
      },
      "outputs": [],
      "source": [
        "best_index = (np.array(silhouette_list) == np.max(np.array(silhouette_list))).nonzero()[0][0]\n",
        "best_silhouette = np.max(np.array(silhouette_list))\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN(**parameter_list[best_index])\n",
        "clusterer.fit(embeddings_inference)\n",
        "\n",
        "# Plotting best results\n",
        "sns.color_palette('Paired', clusterer.labels_.max()+1)\n",
        "\n",
        "color_palette = sns.color_palette('Paired', clusterer.labels_.max()+1)\n",
        "cluster_colors = [color_palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)]\n",
        "cluster_alphas = np.ones_like(clusterer.labels_)*0.5\n",
        "cluster_alphas[clusterer.labels_==-1] = 0.1\n",
        "\n",
        "fig = draw_umap(data=embeddings_inference, colors=cluster_member_colors, n_neighbors=30, min_dist=0.1, title='n_neighbors = {}'.format(15), alpha=cluster_alphas)\n",
        "\n",
        "wandb.log({\"UMAP 2d Optimal silhouette\": wandb.Image(fig)})\n",
        "\n",
        "log_df = pd.DataFrame(parameter_list[best_index],index=[0])\n",
        "best_parameters_table = wandb.Table(dataframe=log_df)\n",
        "wandb.log({\"Best silhouette parameters\": best_parameters_table})\n",
        "\n",
        "parameter_list[best_index]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkFJwQjkVu34"
      },
      "outputs": [],
      "source": [
        "wandb.log({'Best Validity:': best_validity,\n",
        "           'Best Silhouette': best_silhouette,\n",
        "           'Best DBIndex': best_dbindex})\n",
        "\n",
        "print('Silhouette Score (best is 1):', best_silhouette)\n",
        "print('Davies-Bouldin Score (best is 0):', best_dbindex)\n",
        "print('HDBSCAN Relative Validity (higher is better):', best_validity)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipEHfupzbO7m"
      },
      "outputs": [],
      "source": [
        "# fig = draw_umap(data=embeddings_inference, colors=cluster_member_colors, n_neighbors=30, min_dist=0.1, title='n_neighbors = {}'.format(15), n_components=3, alpha=cluster_alphas)\n",
        "\n",
        "# wandb.log({\"UMAP 3d Optimal HDBSCAN\": wandb.Image(fig)})\n",
        "\n",
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB6TGr0im_V3"
      },
      "source": [
        "# Cluster plotting and visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsdJ1K8RmP5f"
      },
      "source": [
        "Init cluster plotting function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHQmMz4nmYTu"
      },
      "outputs": [],
      "source": [
        "def cluster_plot(cluster_rows:int, cluster_col:int, images_list:np.array, dpi:int):\n",
        "  fig = plt.figure(figsize=[cluster_col, cluster_rows], layout='constrained', dpi=dpi)\n",
        "  spec = gridspec.GridSpec(ncols=cluster_col, nrows=cluster_rows, figure=fig, wspace=0, hspace=0)\n",
        "\n",
        "  # Shuffle list and use first 16 filepaths to plot images\n",
        "  np.random.shuffle(images_list)\n",
        "\n",
        "  # For loop to go through and use Team Yellow's load image module\n",
        "  n = 0\n",
        "  for j in range(cluster_rows):\n",
        "    for i in range(cluster_col):\n",
        "      if images_list.shape[0] > n:\n",
        "        image = read_image(image_loc = images_list[n], image_format = \"jpg\")\n",
        "        # Scatter plot\n",
        "        ax1 = fig.add_subplot(spec[j, i])\n",
        "        ax1.imshow(image)\n",
        "        ax1.set_xticks([])\n",
        "        ax1.set_yticks([])\n",
        "      else:\n",
        "        break\n",
        "      n += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tltW46nwl1lj"
      },
      "outputs": [],
      "source": [
        "# Loop through all cluster labels\n",
        "n_clusters_2_plot = 20\n",
        "\n",
        "# Create a random draw of integers between a range\n",
        "clusters_2_plot = np.random.choice(clusterer.labels_[clusterer.labels_ >= 0],\n",
        "                                   size=n_clusters_2_plot,\n",
        "                                   replace=False)\n",
        "\n",
        "for cluster in clusters_2_plot:\n",
        "  cluster_plot(cluster_rows=2,\n",
        "               cluster_col=10,\n",
        "               images_list=returned_images[clusterer.labels_==cluster],\n",
        "               dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c1LBDODQxo9"
      },
      "outputs": [],
      "source": [
        "fit = umap.UMAP()\n",
        "%time u = fit.fit_transform(embeddings_inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o87AApV_REqo"
      },
      "outputs": [],
      "source": [
        "plt.scatter(u[:,0], u[:,1], s=5, linewidth=0, c=cluster_member_colors, alpha=0.25)\n",
        "plt.title('UMAP embedding of six dimensions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGU3kiUhUa3l"
      },
      "source": [
        "Draw Map Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOi7TusLWAh3"
      },
      "outputs": [],
      "source": [
        "draw_umap(data=embeddings_inference, colors=cluster_member_colors, n_neighbors=15, title='n_neighbors = {}'.format(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxHO5YXkU6Go"
      },
      "outputs": [],
      "source": [
        "# for n in (2, 5, 10, 20, 50, 100, 200):\n",
        "#     draw_umap(n_neighbors=n, title='n_neighbors = {}'.format(n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd2w6urb9sTO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
