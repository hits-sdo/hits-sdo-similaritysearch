{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hits-sdo/hits-sdo-similaritysearch/blob/byol_cuml/search_byol/byol_train_pl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSmPQhQA8cPS"
      },
      "source": [
        "# Notebook to Intialize HITS-SDO self-similarity search environment\n",
        "- Run all cells to initalize environment, and restart runtime if prompted to use updated versions. You will need to rerun the cells again to ensure that all dependencies have been installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqLSpbNoO1po"
      },
      "source": [
        "# Download and Unzip Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erAMHVo9NS7U",
        "outputId": "f7e53fb7-83eb-45bc-ebe0-d68f6f4503d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1X8RSekrAOryVGSOeC18NOctfJCQrZCUa\n",
            "To: /content/AIA211_193_171_256x256.tar.gz\n",
            "100% 297M/297M [00:05<00:00, 52.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download Data - Uncomment the one you want\n",
        "\n",
        "# 171 with false yellow colors - https://drive.google.com/file/d/15C5spf1la7L09kvWXll2qt67Ec0rwLsY/view?usp=drive_link\n",
        "# !gdown 15C5spf1la7L09kvWXll2qt67Ec0rwLsY\n",
        "\n",
        "\n",
        "# 171 grayscale - https://drive.google.com/file/d/16WD0td1f5gx4yIIDkWWSTb-oZcezI1CU/view?usp=drive_link\n",
        "# !gdown 16WD0td1f5gx4yIIDkWWSTb-oZcezI1CU\n",
        "\n",
        "\n",
        "# # Multiwavelength r:211 g:193 b:171 https://drive.google.com/file/d/1DMIatOmA4XcoWeW0oAUkZujx8YrhLkpY/view?usp=sharing\n",
        "# !gdown 1DMIatOmA4XcoWeW0oAUkZujx8YrhLkpY\n",
        "# # Multiwavelength r:211 g:193 b:171active cycle extension https://drive.google.com/file/d/1-BPQpdQWNwZzoVpg3lQ143cr3_UPE36M/view?usp=sharing\n",
        "# !gdown 1-BPQpdQWNwZzoVpg3lQ143cr3_UPE36M\n",
        "\n",
        "# Multiwavelength r:211 g:193 b:171 256x256 https://drive.google.com/file/d/1X8RSekrAOryVGSOeC18NOctfJCQrZCUa/view?usp=sharing\n",
        "!gdown 1X8RSekrAOryVGSOeC18NOctfJCQrZCUa\n",
        "\n",
        "\n",
        "# Multiwavelength r:304 g:211 b:171 https://drive.google.com/file/d/1Pts431S-fdSfJJ6pt79-fkNYazXqO9O7/view?usp=sharing\n",
        "# !gdown 1Pts431S-fdSfJJ6pt79-fkNYazXqO9O7\n",
        "# Multiwavelength r:304 g:211 b:171 active cycle extension https://drive.google.com/file/d/1Sbbgi6HzJkuCTUF8tJTLeddpGIjGXTcj/view?usp=sharing\n",
        "# !gdown 1Sbbgi6HzJkuCTUF8tJTLeddpGIjGXTcj\n",
        "\n",
        "\n",
        "# Multiwavelength r:335 g:193 b:94 https://drive.google.com/file/d/1v0IYSzwiQcHPcnSsTAeU7bI8c4310b35/view?usp=sharing\n",
        "# !gdown 1v0IYSzwiQcHPcnSsTAeU7bI8c4310b35\n",
        "# Multiwavelength r:335 g:193 b:94 active cycle extension https://drive.google.com/file/d/1uSt8BzpVj5GgM9SwSsDcQMEE4lv1uTJo/view?usp=sharing\n",
        "# !gdown 1uSt8BzpVj5GgM9SwSsDcQMEE4lv1uTJo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IJYUU6FAOJeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b46e11f-a432-49cc-f02e-77b52f51177a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar (child): AIA211_193_171_Miniset_ext.tar.gz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ],
      "source": [
        "# Unzip file make them match the cell above\n",
        "\n",
        "# 171 with false yellow colors\n",
        "# !tar -zxf aia_171_color_1perMonth.tar.gz\n",
        "\n",
        "\n",
        "# 171 grayscale\n",
        "# !tar -zxf AIA171_Miniset_BW.tar.gz\n",
        "\n",
        "\n",
        "# # Multiwavelength r:211 g:193 b:171\n",
        "# !tar -zxf AIA211_193_171_Miniset.tar.gz\n",
        "# # Multiwavelength r:211 g:193 b:171active cycle extension\n",
        "# !tar -zxf AIA211_193_171_Miniset_ext.tar.gz\n",
        "\n",
        "# Multiwavelength r:211 g:193 b:171 256x256\n",
        "!tar -zxf AIA211_193_171_256x256.tar.gz\n",
        "\n",
        "# Multiwavelength r:304 g:211 b:171\n",
        "# !tar -zxf AIA304_211_171_Miniset.tar.gz\n",
        "# Multiwavelength r:304 g:211 b:171 active cycle extension\n",
        "# !tar -zxf AIA304_211_171_Miniset_ext.tar.gz\n",
        "\n",
        "\n",
        "# Multiwavelength r:335 g:193 b:94\n",
        "# !tar -zxf AIA335_193_94_Miniset.tar.gz\n",
        "# Multiwavelength r:335 g:193 b:94 active cycle extension\n",
        "# !tar -zxf AIA335_193_94_Miniset_ext.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FYwjORgaP2yY"
      },
      "outputs": [],
      "source": [
        "# Print some files to see that they exist - match cells above\n",
        "\n",
        "# 171 with false yellow colors\n",
        "# !du aia_171_color_1perMonth/. -l -h\n",
        "\n",
        "# 171 grayscale\n",
        "# !du AIA171_Miniset_BW/. -l -h\n",
        "\n",
        "# Multiwavelength r:211 g:193 b:171\n",
        "# !du AIA211_193_171_Miniset/. -l -h\n",
        "\n",
        "# Multiwavelength r:304 g:211 b:171\n",
        "# !du AIA304_211_171_Miniset/. -l -h\n",
        "\n",
        "# Multiwavelength r:335 g:193 b:94\n",
        "# !du AIA335_193_94_Miniset/. -l -h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gNxeWeSQRUp"
      },
      "source": [
        "# Clone repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blhqiMyQ8f8c",
        "outputId": "15050272-9ca2-4128-f329-f428301b528c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hits-sdo-similaritysearch'...\n",
            "remote: Enumerating objects: 1643, done.\u001b[K\n",
            "remote: Counting objects: 100% (355/355), done.\u001b[K\n",
            "remote: Compressing objects: 100% (192/192), done.\u001b[K\n",
            "remote: Total 1643 (delta 229), reused 195 (delta 162), pack-reused 1288\u001b[K\n",
            "Receiving objects: 100% (1643/1643), 248.20 MiB | 29.59 MiB/s, done.\n",
            "Resolving deltas: 100% (879/879), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository from GitHub\n",
        "!git clone https://github.com/hits-sdo/hits-sdo-similaritysearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1gZD1xt-uoz",
        "outputId": "b68d2ae5-9459-453b-8710-7a134fb3b0b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hits-sdo-similaritysearch\n"
          ]
        }
      ],
      "source": [
        "%cd hits-sdo-similaritysearch/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZQt3zDtQf1G"
      },
      "source": [
        "# Switch to Desired Branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_Lr1XLG931E",
        "outputId": "70f68af1-b91b-46c5-edab-c4c92b396208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'byol_cuml' set up to track remote branch 'byol_cuml' from 'origin'.\n",
            "Switched to a new branch 'byol_cuml'\n"
          ]
        }
      ],
      "source": [
        "# Switch to the desired branch with requirements.txt\n",
        "!git checkout byol_cuml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-AWACo_j-emk"
      },
      "outputs": [],
      "source": [
        "# Confirm that branch is up to date\n",
        "# !git log --oneline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r65HUQl7QoSM"
      },
      "source": [
        "# Install all necesary packages into environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E01zWPA7fcVL",
        "outputId": "3cba6219-1b3c-47dd-c922-fa852ffe3b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping cudf-cu11 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping cuml-cu11 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y -q -r requirements_cuml.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3AWVW-w9RSV",
        "outputId": "025fbfda-d299-4994-e032-74f47636b270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m637.1/637.1 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsAaEKtCSUX8",
        "outputId": "e629b2b7-2fbc-4cac-cf29-6eb2eb7cae71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.3/489.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 GB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -r requirements_cuml.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EQmyquSd49S"
      },
      "source": [
        "##  Load Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niefkB7Pd49S"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.chdir('/home/amunoz/hits-sdo-similaritysearch')\n",
        "\n",
        "import copy\n",
        "from cuml.cluster import hdbscan\n",
        "from cuml import UMAP\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import pandas as pd\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.colors as colors\n",
        "import matplotlib\n",
        "\n",
        "from lightly.data import LightlyDataset\n",
        "from lightly.data.multi_view_collate import MultiViewCollate\n",
        "from lightly.loss import NegativeCosineSimilarity, NTXentLoss\n",
        "from lightly.models.modules import BYOLPredictionHead, BYOLProjectionHead\n",
        "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
        "from lightly.transforms.simclr_transform import SimCLRTransform\n",
        "from lightly.utils.scheduler import cosine_schedule\n",
        "\n",
        "import seaborn as sns\n",
        "from search_byol.dataset import SDOTilesDataset\n",
        "from search_utils.image_utils import read_image\n",
        "\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "from cuml.metrics.cluster.silhouette_score import cython_silhouette_score as silhouette_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AeUD0yxW5-h"
      },
      "source": [
        "## Login to Wanddb and initialize logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_Xla6gJX8Lh"
      },
      "outputs": [],
      "source": [
        "!wandb login --relogin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZuoKah_dWpe"
      },
      "source": [
        "## Define run parameters and initalize Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Car-gZLeMUiW"
      },
      "outputs": [],
      "source": [
        "# Set data type and data path.  Make sure it matches cells above\n",
        "\n",
        "# 171 with false yellow colors\n",
        "# data_type = '171Color'\n",
        "# data_path = '/content/aia_171_color_1perMonth'\n",
        "\n",
        "# 171 grayscale\n",
        "# data_type = '171gray'\n",
        "# data_path = '/content/AIA171_Miniset_BW'\n",
        "\n",
        "# Multiwavelength r:211 g:193 b:171\n",
        "# data_type = 'r:211 g:193 b:171'\n",
        "# data_type = 'r:211 g:193 b:171 extended'\n",
        "# data_path = '/content/AIA211_193_171_Miniset'\n",
        "# data_path = '/mnt/d/Mis Documentos/AAResearch/SEARCH/hits-sdo-downloader/AIA211_193_171_Miniset_exp/AIA211_193_171_Miniset'\n",
        "\n",
        "# Multiwavelength r:211 g:193 b:171 256x256\n",
        "data_type = 'r:211 g:193 b:171 256x256'\n",
        "data_path = '/content/AIA211_193_171_256x256'\n",
        "\n",
        "# Multiwavelength r:304 g:211 b:171\n",
        "# data_type = 'r:304 g:211 b:171'\n",
        "# data_type = 'r:304 g:211 b:171 extended'\n",
        "# data_path = '/content/AIA304_211_171_Miniset'\n",
        "\n",
        "# Multiwavelength r:335 g:193 b:94\n",
        "# data_type = 'r:335 g:193 b:94'\n",
        "# data_type = 'r:335 g:193 b:94 extended'\n",
        "# data_path = '/content/AIA335_193_94_Miniset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEDgOBn4dfyn"
      },
      "outputs": [],
      "source": [
        "seed = 42 # So clever.\n",
        "pl.seed_everything(seed, workers=True)\n",
        "\n",
        "initial = \"AMJ\"\n",
        "job_type = \"HDB-SCAN-Sweep_Cuml_256\"\n",
        "epochs = 16\n",
        "data_stride = 1\n",
        "batch_size = 64\n",
        "augmentation = 'double'\n",
        "loss = 'contrast'   # 'contrast' or 'cos'\n",
        "learning_rate = 0.1\n",
        "cosine_scheduler_start = .1\n",
        "cosine_scheduler_end = 1.0\n",
        "projection_size = 32\n",
        "prediction_size = 32\n",
        "\n",
        "notes = \"\"\n",
        "name = f\"{initial}-ds{data_stride}_bs{batch_size}_lr{learning_rate}_{augmentation}aug_ss{cosine_scheduler_start}_se{cosine_scheduler_end}_pjs{projection_size}_pds{prediction_size}_{loss}\"\n",
        "group = \"clustering\"\n",
        "tags = [\"experimentation\", \"checkpoint\"]\n",
        "\n",
        "\n",
        "wandb_logger = WandbLogger(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"search-byol-for_real\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"seed\": seed,\n",
        "    \"data_type\": data_type,\n",
        "    \"batch size\": batch_size,\n",
        "    \"augmention\": augmentation,\n",
        "    \"data stride\": data_stride,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"cosine_scheduler_start\": cosine_scheduler_start,\n",
        "    \"cosine_scheduler_end\": cosine_scheduler_end\n",
        "    },\n",
        "    entity = \"search-byol\",\n",
        "    job_type = job_type,\n",
        "    name = name,\n",
        "    notes = notes,\n",
        "    group = group,\n",
        "    tags = tags,\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTTNr9Qad49S"
      },
      "source": [
        "## Define BYOL Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl8JlRFfd49S"
      },
      "outputs": [],
      "source": [
        "class BYOL(pl.LightningModule):\n",
        "    def __init__(self, lr=0.1, projection_size=256, prediction_size=256, cosine_scheduler_start=0.1, cosine_scheduler_end=1.0, epochs=10, loss='cos'):\n",
        "        super().__init__()\n",
        "\n",
        "        resnet = torchvision.models.resnet18() # Play w/ resnet.\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.projection_head = BYOLProjectionHead(512, 1024, projection_size)\n",
        "        self.prediction_head = BYOLPredictionHead(projection_size, 1024, prediction_size)\n",
        "\n",
        "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
        "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
        "\n",
        "        deactivate_requires_grad(self.backbone_momentum)\n",
        "        deactivate_requires_grad(self.projection_head_momentum)\n",
        "\n",
        "\n",
        "        self.loss = loss\n",
        "        self.loss_cos = NegativeCosineSimilarity()\n",
        "        self.loss_contrast = NTXentLoss()\n",
        "\n",
        "        self.cosine_scheduler_start = cosine_scheduler_start\n",
        "        self.cosine_scheduler_end = cosine_scheduler_end\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.backbone(x).flatten(start_dim=1)\n",
        "        z = self.projection_head(y)\n",
        "        p = self.prediction_head(z)\n",
        "        return p\n",
        "\n",
        "    def forward_momentum(self, x):\n",
        "        y = self.backbone_momentum(x).flatten(start_dim=1)\n",
        "        z = self.projection_head_momentum(y)\n",
        "        z = z.detach()\n",
        "        return z\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "\n",
        "        momentum = cosine_schedule(self.current_epoch, self.epochs, self.cosine_scheduler_start, self.cosine_scheduler_end)\n",
        "        update_momentum(self.backbone, self.backbone_momentum, m=momentum)\n",
        "        update_momentum(self.projection_head, self.projection_head_momentum, m=momentum)\n",
        "        (x0, x1, _) = batch\n",
        "        p0 = self.forward(x0)\n",
        "        z0 = self.forward_momentum(x0)\n",
        "        p1 = self.forward(x1)\n",
        "        z1 = self.forward_momentum(x1)\n",
        "\n",
        "        loss_cos = 0.5 * (self.loss_cos(p0, z1) + self.loss_cos(p1, z0))\n",
        "        loss_contrast = 0.5 * (self.loss_contrast(p0, z1) + self.loss_contrast(p1, z0))\n",
        "\n",
        "        if self.loss == 'cos':\n",
        "            loss = loss_cos\n",
        "        else:\n",
        "            loss = loss_contrast\n",
        "\n",
        "        self.log('loss cos', loss_cos)\n",
        "        self.log('loss contrast', loss_contrast)\n",
        "        self.log('loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SGD(self.parameters(), lr=self.lr) # Play w/ optimizers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOaS3xWrd49T"
      },
      "source": [
        "## Initialize Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4tn1MpKd49T"
      },
      "outputs": [],
      "source": [
        "model = BYOL(lr=learning_rate, projection_size=projection_size, prediction_size=prediction_size, cosine_scheduler_start=cosine_scheduler_start, cosine_scheduler_end=cosine_scheduler_end, loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh9kGF4Ad49T"
      },
      "source": [
        "## Initialize Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEryt3YUd49T"
      },
      "outputs": [],
      "source": [
        "dataset = SDOTilesDataset(data_path=data_path, augmentation=augmentation, data_stride=data_stride)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        "    num_workers=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oaOqFlRd49U"
      },
      "source": [
        "## Run Training Loop, Inference, and log in wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSMdSelud49U"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(max_epochs=epochs,\n",
        "                     accelerator=\"auto\", devices=\"auto\", strategy=\"auto\",\n",
        "                     logger=wandb_logger, log_every_n_steps=10, deterministic=True)\n",
        "\n",
        "wandb_logger.watch(model)\n",
        "trainer.fit(model=model, train_dataloaders=dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGx0ROM5QBdF"
      },
      "source": [
        "### Save model to wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz6ik6KSLQ7L"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), f'{name}_.pt')\n",
        "wandb.save(f'{name}_.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUcoNAuvQGAK"
      },
      "source": [
        "## Run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Jj9mNEbOth2"
      },
      "outputs": [],
      "source": [
        "dataset_inference = SDOTilesDataset(data_path=data_path, augmentation='none', data_stride=1)\n",
        "\n",
        "dataloader_inference = torch.utils.data.DataLoader(\n",
        "    dataset_inference,\n",
        "    batch_size=1000,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        "    num_workers=8,\n",
        ")\n",
        "\n",
        "batches_bar = tqdm(dataloader_inference, dynamic_ncols=True, leave=True, desc=f'Running inference on batches')\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "inference_list = []\n",
        "filename_list = []\n",
        "\n",
        "for (x0, image_file) in batches_bar:\n",
        "    embeddings_inference = model.forward_momentum(x0.to(device)).cpu().numpy()\n",
        "    inference_list.append(embeddings_inference)\n",
        "    filename_list.append(image_file)\n",
        "\n",
        "embeddings_inference = np.concatenate(inference_list, axis=0)\n",
        "returned_images = np.concatenate(filename_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_Z4KFgzkW6n"
      },
      "source": [
        "# Cluster evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTS1mnskWLnV"
      },
      "source": [
        "### Relative validity calculation taken from HDBSCAN cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQMRlWtmWLnW"
      },
      "outputs": [],
      "source": [
        "def relative_validity(labels_, minimum_spanning_tree_):\n",
        "\n",
        "    labels = labels_\n",
        "    sizes = np.bincount(labels + 1)\n",
        "    noise_size = sizes[0]\n",
        "    cluster_size = sizes[1:]\n",
        "    total = noise_size + np.sum(cluster_size)\n",
        "    num_clusters = len(cluster_size)\n",
        "    DSC = np.zeros(num_clusters)\n",
        "    min_outlier_sep = np.inf  # only required if num_clusters = 1\n",
        "    correction_const = 2  # only required if num_clusters = 1\n",
        "\n",
        "    # Unltimately, for each Ci, we only require the\n",
        "    # minimum of DSPC(Ci, Cj) over all Cj != Ci.\n",
        "    # So let's call this value DSPC_wrt(Ci), i.e.\n",
        "    # density separation 'with respect to' Ci.\n",
        "    DSPC_wrt = np.ones(num_clusters) * np.inf\n",
        "    max_distance = 0\n",
        "\n",
        "    mst_df = minimum_spanning_tree_.to_pandas()\n",
        "\n",
        "    for edge in mst_df.iterrows():\n",
        "        label1 = labels[int(edge[1][\"from\"])]\n",
        "        label2 = labels[int(edge[1][\"to\"])]\n",
        "        length = edge[1][\"distance\"]\n",
        "\n",
        "        max_distance = max(max_distance, length)\n",
        "\n",
        "        if label1 == -1 and label2 == -1:\n",
        "            continue\n",
        "        elif label1 == -1 or label2 == -1:\n",
        "            # If exactly one of the points is noise\n",
        "            min_outlier_sep = min(min_outlier_sep, length)\n",
        "            continue\n",
        "\n",
        "        if label1 == label2:\n",
        "            # Set the density sparseness of the cluster\n",
        "            # to the sparsest value seen so far.\n",
        "            DSC[label1] = max(length, DSC[label1])\n",
        "        else:\n",
        "            # Check whether density separations with\n",
        "            # respect to each of these clusters can\n",
        "            # be reduced.\n",
        "            DSPC_wrt[label1] = min(length, DSPC_wrt[label1])\n",
        "            DSPC_wrt[label2] = min(length, DSPC_wrt[label2])\n",
        "\n",
        "    # In case min_outlier_sep is still np.inf, we assign a new value to it.\n",
        "    # This only makes sense if num_clusters = 1 since it has turned out\n",
        "    # that the MR-MST has no edges between a noise point and a core point.\n",
        "    min_outlier_sep = max_distance if min_outlier_sep == np.inf else min_outlier_sep\n",
        "\n",
        "    # DSPC_wrt[Ci] might be infinite if the connected component for Ci is\n",
        "    # an \"island\" in the MR-MST. Whereas for other clusters Cj and Ck, the\n",
        "    # MR-MST might contain an edge with one point in Cj and ther other one\n",
        "    # in Ck. Here, we replace the infinite density separation of Ci by\n",
        "    # another large enough value.\n",
        "    #\n",
        "    # TODO: Think of a better yet efficient way to handle this.\n",
        "    correction = correction_const * (\n",
        "        max_distance if num_clusters > 1 else min_outlier_sep\n",
        "    )\n",
        "    DSPC_wrt[np.where(DSPC_wrt == np.inf)] = correction\n",
        "\n",
        "    V_index = [\n",
        "        (DSPC_wrt[i] - DSC[i]) / max(DSPC_wrt[i], DSC[i])\n",
        "        for i in range(num_clusters)\n",
        "    ]\n",
        "    score = np.sum(\n",
        "        [(cluster_size[i] * V_index[i]) / total for i in range(num_clusters)]\n",
        "    )\n",
        "    _relative_validity = score\n",
        "    return _relative_validity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFq6WpJOgwji"
      },
      "outputs": [],
      "source": [
        "# specify parameters and distributions to sample from\n",
        "param_dist = {'min_samples': [2],\n",
        "              'min_cluster_size':[2, 4, 8, 16, 32, 64, 128, 256],\n",
        "              'cluster_selection_method' : ['leaf'],\n",
        "              'cluster_selection_epsilon' : [0.0, 0.1, 0.2, 0.3],\n",
        "              'metric' : ['euclidean'],\n",
        "              'gen_min_span_tree' : [True]\n",
        "             }\n",
        "\n",
        "wandb.config.update(param_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnCUW32MHXYy"
      },
      "source": [
        "### Normalizing embeddings so that Euclidean distance corresponds to Cosine distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XKFZK8KHGv9"
      },
      "outputs": [],
      "source": [
        "normalized_embeddings = embeddings_inference/norm(embeddings_inference, axis=1, ord=2)[:,None]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb0OXzRubdlx"
      },
      "source": [
        "**IMPORTANT**: If the program crashes with `CUDADriverError: CUDA_ERROR_INVALID_VALUE: invalid argument` it means you are running out of VRAM during the silhouette calculation and need to increase the stride in the calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mHiZwW4Lt1l"
      },
      "outputs": [],
      "source": [
        "#Increase this number if you run out of memory (see above)\n",
        "silhouette_stride = 10\n",
        "\n",
        "parameter_list =  list(ParameterGrid(param_dist))\n",
        "relative_validity_list = []\n",
        "DBIndex_list = []\n",
        "silhouette_list = []\n",
        "\n",
        "best_validity=-1\n",
        "best_dbindex=-1\n",
        "best_silhouette=-1\n",
        "parameters_bar = tqdm(ParameterGrid(param_dist), desc=f'Best relative validity {best_validity:>02}, Best DBindex {best_dbindex:>02}, Best silhouette {best_silhouette:>02}')\n",
        "for parameters in parameters_bar:\n",
        "  clusterer = hdbscan.HDBSCAN(**parameters)\n",
        "  clusterer.fit(normalized_embeddings)\n",
        "  relative_validity_list.append(relative_validity(clusterer.labels_, clusterer.minimum_spanning_tree_))\n",
        "\n",
        "  ss = silhouette_score(normalized_embeddings[0::silhouette_stride,:], clusterer.labels_[0::silhouette_stride], metric='euclidean')\n",
        "  silhouette_list.append(ss)\n",
        "\n",
        "  dbs = davies_bouldin_score(normalized_embeddings, clusterer.labels_)\n",
        "  DBIndex_list.append(dbs)\n",
        "\n",
        "  best_validity = np.max(np.array(relative_validity_list))\n",
        "  best_dbindex = np.min(np.array(DBIndex_list))\n",
        "  best_silhouette= np.max(np.array(silhouette_list))\n",
        "\n",
        "  parameters_bar.set_description(f'Best relative validity {best_validity:>02}, Best DBindex {best_dbindex:>02}, Best silhouette {best_silhouette:>02}')\n",
        "  parameters_bar.refresh()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M7t0tQMWLnZ"
      },
      "source": [
        "## Visualize best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbOC-lCbWLna"
      },
      "outputs": [],
      "source": [
        "n_neighbors=5\n",
        "min_dist=0.0\n",
        "n_components=2\n",
        "metric='cosine'\n",
        "spread = 0.5\n",
        "repulsion_strength = 2\n",
        "\n",
        "def draw_umap(data, colors, n_neighbors=15, min_dist=0.1, spread=1, n_components=2, repulsion_strength=1.0, metric='euclidean', title='', alpha=0.1):\n",
        "    fit = UMAP(\n",
        "        n_neighbors=n_neighbors,\n",
        "        min_dist=min_dist,\n",
        "        n_components=n_components,\n",
        "        metric=metric,\n",
        "        spread=spread,\n",
        "        repulsion_strength=repulsion_strength,\n",
        "        verbose=True\n",
        "    )\n",
        "    u = fit.fit_transform(data);\n",
        "    fig = plt.figure(figsize=(16,9), dpi=150)\n",
        "    if n_components == 1:\n",
        "        ax = fig.add_subplot(111)\n",
        "        ax.scatter(u[:,0], range(len(u)), c=colors, s=1, alpha=alpha)\n",
        "    if n_components == 2:\n",
        "        ax = fig.add_subplot(111)\n",
        "        ax.scatter(u[:,0], u[:,1], c=colors, s=1, alpha=alpha)\n",
        "    if n_components == 3:\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.scatter(u[:,0], u[:,1], u[:,2], c=colors, s=1, alpha=alpha)\n",
        "    plt.title(title, fontsize=18)\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvUF1TH_VCpq"
      },
      "outputs": [],
      "source": [
        "best_index = (np.array(relative_validity_list) == np.max(np.array(relative_validity_list))).nonzero()[0][0]\n",
        "best_validity = np.max(np.array(relative_validity_list))\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN(**parameter_list[best_index])\n",
        "clusterer.fit(normalized_embeddings)\n",
        "\n",
        "# Plotting best results\n",
        "sns.color_palette('Paired', clusterer.labels_.max()+1)\n",
        "\n",
        "color_palette = sns.color_palette('Paired', clusterer.labels_.max()+1)\n",
        "cluster_colors = [color_palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)]\n",
        "cluster_alphas = np.ones_like(clusterer.labels_)*0.5\n",
        "cluster_alphas[clusterer.labels_==-1] = 0.1\n",
        "\n",
        "fig = draw_umap(data=normalized_embeddings, colors=cluster_member_colors, n_neighbors=n_neighbors, min_dist=min_dist, repulsion_strength=repulsion_strength, n_components=n_components, spread=spread, metric=metric, title='', alpha=cluster_alphas)\n",
        "\n",
        "wandb.log({\"UMAP 2d Optimal validity\": wandb.Image(fig)})\n",
        "\n",
        "log_df = pd.DataFrame(parameter_list[best_index],index=[0])\n",
        "best_parameters_table = wandb.Table(dataframe=log_df)\n",
        "wandb.log({\"Best validity parameters\": best_parameters_table})\n",
        "\n",
        "parameter_list[best_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td9U56_lh-7n"
      },
      "outputs": [],
      "source": [
        "best_index = (np.array(DBIndex_list) == np.min(np.array(DBIndex_list))).nonzero()[0][0]\n",
        "best_dbindex = np.min(np.array(DBIndex_list))\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN(**parameter_list[best_index])\n",
        "clusterer.fit(normalized_embeddings)\n",
        "\n",
        "# Plotting best results\n",
        "sns.color_palette('Paired', clusterer.labels_.max()+1)\n",
        "\n",
        "color_palette = sns.color_palette('Paired', clusterer.labels_.max()+1)\n",
        "cluster_colors = [color_palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)]\n",
        "cluster_alphas = np.ones_like(clusterer.labels_)*0.5\n",
        "cluster_alphas[clusterer.labels_==-1] = 0.1\n",
        "\n",
        "fig = draw_umap(data=normalized_embeddings, colors=cluster_member_colors, n_neighbors=n_neighbors, min_dist=min_dist, repulsion_strength=repulsion_strength, n_components=n_components, spread=spread, metric=metric, title='', alpha=cluster_alphas)\n",
        "\n",
        "wandb.log({\"UMAP 2d Optimal DBIndex\": wandb.Image(fig)})\n",
        "\n",
        "log_df = pd.DataFrame(parameter_list[best_index],index=[0])\n",
        "best_parameters_table = wandb.Table(dataframe=log_df)\n",
        "wandb.log({\"Best DBIndex parameters\": best_parameters_table})\n",
        "\n",
        "parameter_list[best_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HViNFj9ziZiZ"
      },
      "outputs": [],
      "source": [
        "best_index = (np.array(silhouette_list) == np.max(np.array(silhouette_list))).nonzero()[0][0]\n",
        "best_silhouette = np.max(np.array(silhouette_list))\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN(**parameter_list[best_index])\n",
        "clusterer.fit(normalized_embeddings)\n",
        "\n",
        "# Plotting best results\n",
        "sns.color_palette('Paired', clusterer.labels_.max()+1)\n",
        "\n",
        "color_palette = sns.color_palette('Paired', clusterer.labels_.max()+1)\n",
        "cluster_colors = [color_palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)]\n",
        "cluster_alphas = np.ones_like(clusterer.labels_)*0.5\n",
        "cluster_alphas[clusterer.labels_==-1] = 0.1\n",
        "\n",
        "\n",
        "fig = draw_umap(data=normalized_embeddings, colors=cluster_member_colors, n_neighbors=n_neighbors, min_dist=min_dist, repulsion_strength=repulsion_strength, n_components=n_components, spread=spread, metric=metric, title='', alpha=cluster_alphas)\n",
        "\n",
        "wandb.log({\"UMAP 2d Optimal silhouette\": wandb.Image(fig)})\n",
        "\n",
        "log_df = pd.DataFrame(parameter_list[best_index],index=[0])\n",
        "best_parameters_table = wandb.Table(dataframe=log_df)\n",
        "wandb.log({\"Best silhouette parameters\": best_parameters_table})\n",
        "\n",
        "parameter_list[best_index]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkFJwQjkVu34"
      },
      "outputs": [],
      "source": [
        "wandb.log({'Best Validity:': best_validity,\n",
        "           'Best Silhouette': best_silhouette,\n",
        "           'Best DBIndex': best_dbindex})\n",
        "\n",
        "print('Silhouette Score (best is 1):', best_silhouette)\n",
        "print('Davies-Bouldin Score (best is 0):', best_dbindex)\n",
        "print('HDBSCAN Relative Validity (higher is better):', best_validity)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipEHfupzbO7m"
      },
      "outputs": [],
      "source": [
        "# fig = draw_umap(data=embeddings_inference, colors=cluster_member_colors, n_neighbors=30, min_dist=0.1, title='n_neighbors = {}'.format(15), n_components=3, alpha=cluster_alphas)\n",
        "\n",
        "# wandb.log({\"UMAP 3d Optimal HDBSCAN\": wandb.Image(fig)})\n",
        "\n",
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB6TGr0im_V3"
      },
      "source": [
        "# Cluster plotting and visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsdJ1K8RmP5f"
      },
      "source": [
        "Init cluster plotting function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHQmMz4nmYTu"
      },
      "outputs": [],
      "source": [
        "def cluster_plot(cluster_rows:int, cluster_col:int, images_list:np.array, dpi:int):\n",
        "  fig = plt.figure(figsize=[cluster_col, cluster_rows], layout='constrained', dpi=dpi)\n",
        "  spec = gridspec.GridSpec(ncols=cluster_col, nrows=cluster_rows, figure=fig, wspace=0, hspace=0)\n",
        "\n",
        "  # Shuffle list and use first 16 filepaths to plot images\n",
        "  np.random.shuffle(images_list)\n",
        "\n",
        "  # For loop to go through and use Team Yellow's load image module\n",
        "  n = 0\n",
        "  for j in range(cluster_rows):\n",
        "    for i in range(cluster_col):\n",
        "      if images_list.shape[0] > n:\n",
        "        image = read_image(image_loc = images_list[n], image_format = \"jpg\")\n",
        "        # Scatter plot\n",
        "        ax1 = fig.add_subplot(spec[j, i])\n",
        "        ax1.imshow(image)\n",
        "        ax1.set_xticks([])\n",
        "        ax1.set_yticks([])\n",
        "      else:\n",
        "        break\n",
        "      n += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tltW46nwl1lj"
      },
      "outputs": [],
      "source": [
        "# Loop through all cluster labels\n",
        "n_clusters_2_plot = 20\n",
        "\n",
        "# Create a random draw of integers between a range\n",
        "clusters_2_plot = np.random.choice(clusterer.labels_[clusterer.labels_ >= 0],\n",
        "                                   size=n_clusters_2_plot,\n",
        "                                   replace=False)\n",
        "\n",
        "for cluster in clusters_2_plot:\n",
        "  cluster_plot(cluster_rows=2,\n",
        "               cluster_col=10,\n",
        "               images_list=returned_images[clusterer.labels_==cluster],\n",
        "               dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c1LBDODQxo9"
      },
      "outputs": [],
      "source": [
        "fit = UMAP()\n",
        "%time u = fit.fit_transform(normalized_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o87AApV_REqo"
      },
      "outputs": [],
      "source": [
        "plt.scatter(u[:,0], u[:,1], s=5, linewidth=0, c=cluster_member_colors, alpha=0.25)\n",
        "plt.title('UMAP embedding of six dimensions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGU3kiUhUa3l"
      },
      "source": [
        "Draw Map Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOi7TusLWAh3"
      },
      "outputs": [],
      "source": [
        "draw_umap(data=normalized_embeddings, colors=cluster_member_colors, n_neighbors=15, title='n_neighbors = {}'.format(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxHO5YXkU6Go"
      },
      "outputs": [],
      "source": [
        "# for n in (2, 5, 10, 20, 50, 100, 200):\n",
        "#     draw_umap(n_neighbors=n, title='n_neighbors = {}'.format(n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd2w6urb9sTO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}